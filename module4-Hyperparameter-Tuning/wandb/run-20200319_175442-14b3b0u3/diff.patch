diff --git a/Warmups.ipynb b/Warmups.ipynb
index fbf1f01..0dccc35 100644
--- a/Warmups.ipynb
+++ b/Warmups.ipynb
@@ -1,8 +1,25 @@
 {
  "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Anika wants to run a Secret Santa for Christmas but has a lot of friends and doesn't want to have to assign everyone by hand. Write a function that takes in an ordered list of names, shuffles it, and then assigns each person a Secret Santa. Per the rules of Secret Santa, no one should be receiving from someone they gave to.\n",
+    "\n",
+    "```Example:\n",
+    "santas = ['a','b','c','d','e']\n",
+    "secret_santafy(santas) ->\n",
+    "c b\n",
+    "b e\n",
+    "e a\n",
+    "a d\n",
+    "d c\n",
+    "```"
+   ]
+  },
   {
    "cell_type": "code",
-   "execution_count": 41,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -11,7 +28,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 42,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -26,16 +43,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 43,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "KEITH CONNOR\n",
-      "CONNOR JANE\n",
-      "JANE JIMMY\n",
+      "KEITH JANE\n",
+      "JANE CONNOR\n",
+      "CONNOR JIMMY\n",
       "JIMMY MICHAEL\n",
       "MICHAEL KEITH\n"
      ]
@@ -45,6 +62,64 @@
     "names = ['KEITH', 'JANE', 'MICHAEL', 'JIMMY', 'CONNOR']\n",
     "santa(names)"
    ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Write a function that takes in a non-empty array of distinct integers and an integer representing a target sum. If any two numbers in the input array sum up to the target sum, the function should return them in an array, in sorted order. If no two numbers sum up to the target sum, the function should return an empty array.\n",
+    "Assume that there will be at most one pair of numbers summing up to the target sum.\n",
+    "\n",
+    "Sample input: [3,5,-4,8,11,1,-1,6], 10\n",
+    "\n",
+    "Sample output: [-1,11]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def sum_finder(input_list=None, sum_total=0):\n",
+    "    pairs = []\n",
+    "    for i in input_list:\n",
+    "        for j in input_list:\n",
+    "            if i + j == sum_total:\n",
+    "                print(i, j)\n",
+    "                pairs.append((i, j))\n",
+    "    \n",
+    "    return pairs"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "5 5\n",
+      "11 -1\n",
+      "-1 11\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[(5, 5), (11, -1), (-1, 11)]"
+      ]
+     },
+     "execution_count": 7,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "sum_finder([3,5,-4,8,11,1,-1,6], 10)"
+   ]
   }
  ],
  "metadata": {
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
index 9c59d8c..a00b42b 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
@@ -91,7 +91,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index 4bb18e9..95ee0a0 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -21,10 +21,7 @@
     "id": "41TS0Sa0rDNx"
    },
    "source": [
-    "# Neural Networks & GPUs (Prepare)\n",
-    "*aka Hyperparameter Tuning*\n",
-    "\n",
-    "*aka Big Servers for Big Problems*"
+    "# Neural Networks Hyperparameter Tuning (Prepare)"
    ]
   },
   {
@@ -36,8 +33,7 @@
    "source": [
     "## Learning Objectives\n",
     "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparemeters to tune\n",
-    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
-    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch (Optional)"
+    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework"
    ]
   },
   {
@@ -47,7 +43,7 @@
    "outputs": [],
    "source": [
     "wandb_group = \"ds8\"\n",
-    "wandb_project = \"inclass\""
+    "wandb_project = \"ds11_inclass\""
    ]
   },
   {
@@ -81,7 +77,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -106,7 +102,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
@@ -170,7 +166,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -187,164 +183,164 @@
      "text": [
       "Train on 404 samples, validate on 102 samples\n",
       "Epoch 1/75\n",
-      "404/404 [==============================] - 2s 4ms/sample - loss: 498.2045 - mse: 498.2046 - mae: 20.2543 - val_loss: 421.5039 - val_mse: 421.5038 - val_mae: 18.3349\n",
+      "404/404 [==============================] - 0s 1ms/sample - loss: 505.6847 - mse: 505.6847 - mae: 20.6552 - val_loss: 425.0313 - val_mse: 425.0313 - val_mae: 18.7985\n",
       "Epoch 2/75\n",
-      "404/404 [==============================] - 0s 347us/sample - loss: 249.6985 - mse: 249.6985 - mae: 13.2672 - val_loss: 111.3743 - val_mse: 111.3743 - val_mae: 8.6210\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 266.5902 - mse: 266.5903 - mae: 13.7868 - val_loss: 135.0595 - val_mse: 135.0595 - val_mae: 9.6444\n",
       "Epoch 3/75\n",
-      "404/404 [==============================] - 0s 344us/sample - loss: 56.6755 - mse: 56.6755 - mae: 5.4817 - val_loss: 39.1997 - val_mse: 39.1997 - val_mae: 4.9872\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 82.8211 - mse: 82.8211 - mae: 6.7614 - val_loss: 60.0892 - val_mse: 60.0892 - val_mae: 6.0737\n",
       "Epoch 4/75\n",
-      "404/404 [==============================] - 0s 364us/sample - loss: 28.3243 - mse: 28.3243 - mae: 3.7054 - val_loss: 26.9866 - val_mse: 26.9866 - val_mae: 4.0796\n",
+      "404/404 [==============================] - 0s 182us/sample - loss: 44.5249 - mse: 44.5249 - mae: 4.8397 - val_loss: 36.9470 - val_mse: 36.9470 - val_mae: 4.7310\n",
       "Epoch 5/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 20.5281 - mse: 20.5281 - mae: 3.1209 - val_loss: 24.6172 - val_mse: 24.6172 - val_mae: 3.8052\n",
+      "404/404 [==============================] - 0s 174us/sample - loss: 29.6820 - mse: 29.6820 - mae: 3.8556 - val_loss: 29.0264 - val_mse: 29.0264 - val_mae: 4.3409\n",
       "Epoch 6/75\n",
-      "404/404 [==============================] - 0s 393us/sample - loss: 17.9283 - mse: 17.9283 - mae: 2.8665 - val_loss: 23.6524 - val_mse: 23.6524 - val_mae: 3.6746\n",
+      "404/404 [==============================] - 0s 170us/sample - loss: 24.2863 - mse: 24.2863 - mae: 3.4391 - val_loss: 25.3930 - val_mse: 25.3930 - val_mae: 4.0905\n",
       "Epoch 7/75\n",
-      "404/404 [==============================] - 0s 440us/sample - loss: 16.9179 - mse: 16.9179 - mae: 2.8781 - val_loss: 23.4620 - val_mse: 23.4620 - val_mae: 3.5778\n",
+      "404/404 [==============================] - 0s 176us/sample - loss: 21.2665 - mse: 21.2665 - mae: 3.2485 - val_loss: 24.1032 - val_mse: 24.1032 - val_mae: 3.9488\n",
       "Epoch 8/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 15.1579 - mse: 15.1579 - mae: 2.6440 - val_loss: 24.1374 - val_mse: 24.1374 - val_mae: 3.5929\n",
+      "404/404 [==============================] - 0s 175us/sample - loss: 19.2692 - mse: 19.2692 - mae: 3.0978 - val_loss: 23.3955 - val_mse: 23.3955 - val_mae: 3.8186\n",
       "Epoch 9/75\n",
-      "404/404 [==============================] - 0s 367us/sample - loss: 14.1717 - mse: 14.1717 - mae: 2.5937 - val_loss: 24.4829 - val_mse: 24.4829 - val_mae: 3.5639\n",
+      "404/404 [==============================] - 0s 172us/sample - loss: 17.5932 - mse: 17.5932 - mae: 2.9636 - val_loss: 21.9165 - val_mse: 21.9165 - val_mae: 3.6342\n",
       "Epoch 10/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 13.5002 - mse: 13.5002 - mae: 2.5633 - val_loss: 25.0170 - val_mse: 25.0170 - val_mae: 3.5601\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 16.6885 - mse: 16.6885 - mae: 2.9046 - val_loss: 22.6805 - val_mse: 22.6805 - val_mae: 3.6678\n",
       "Epoch 11/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.8641 - mse: 12.8641 - mae: 2.4963 - val_loss: 25.1162 - val_mse: 25.1162 - val_mae: 3.5449\n",
+      "404/404 [==============================] - 0s 179us/sample - loss: 15.1097 - mse: 15.1097 - mae: 2.7201 - val_loss: 21.5856 - val_mse: 21.5856 - val_mae: 3.4821\n",
       "Epoch 12/75\n",
-      "404/404 [==============================] - 0s 351us/sample - loss: 12.4033 - mse: 12.4033 - mae: 2.5224 - val_loss: 25.0382 - val_mse: 25.0382 - val_mae: 3.4858\n",
+      "404/404 [==============================] - 0s 178us/sample - loss: 14.2978 - mse: 14.2978 - mae: 2.6384 - val_loss: 22.3463 - val_mse: 22.3463 - val_mae: 3.4898\n",
       "Epoch 13/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.2653 - mse: 12.2653 - mae: 2.4637 - val_loss: 26.7274 - val_mse: 26.7274 - val_mae: 3.6054\n",
+      "404/404 [==============================] - 0s 172us/sample - loss: 13.5027 - mse: 13.5027 - mae: 2.6010 - val_loss: 21.7838 - val_mse: 21.7838 - val_mae: 3.3946\n",
       "Epoch 14/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 11.8249 - mse: 11.8249 - mae: 2.4648 - val_loss: 25.2347 - val_mse: 25.2347 - val_mae: 3.4602\n",
+      "404/404 [==============================] - 0s 171us/sample - loss: 12.9157 - mse: 12.9157 - mae: 2.5296 - val_loss: 21.8595 - val_mse: 21.8595 - val_mae: 3.3729\n",
       "Epoch 15/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 11.3965 - mse: 11.3965 - mae: 2.4134 - val_loss: 25.3070 - val_mse: 25.3070 - val_mae: 3.4305\n",
+      "404/404 [==============================] - 0s 168us/sample - loss: 12.5167 - mse: 12.5167 - mae: 2.4960 - val_loss: 21.6965 - val_mse: 21.6965 - val_mae: 3.2943\n",
       "Epoch 16/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 11.0982 - mse: 11.0982 - mae: 2.3616 - val_loss: 25.0599 - val_mse: 25.0599 - val_mae: 3.3784\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 11.5885 - mse: 11.5885 - mae: 2.4052 - val_loss: 21.4154 - val_mse: 21.4154 - val_mae: 3.2717\n",
       "Epoch 17/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 11.1969 - mse: 11.1969 - mae: 2.3806 - val_loss: 25.1976 - val_mse: 25.1976 - val_mae: 3.3732\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 11.2657 - mse: 11.2657 - mae: 2.3723 - val_loss: 20.6518 - val_mse: 20.6518 - val_mae: 3.1602\n",
       "Epoch 18/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 10.9278 - mse: 10.9278 - mae: 2.3653 - val_loss: 24.2875 - val_mse: 24.2875 - val_mae: 3.3114\n",
+      "404/404 [==============================] - 0s 195us/sample - loss: 10.9611 - mse: 10.9611 - mae: 2.3493 - val_loss: 22.2263 - val_mse: 22.2263 - val_mae: 3.2351\n",
       "Epoch 19/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 10.5854 - mse: 10.5854 - mae: 2.3170 - val_loss: 26.1450 - val_mse: 26.1450 - val_mae: 3.3971\n",
+      "404/404 [==============================] - 0s 210us/sample - loss: 10.5958 - mse: 10.5958 - mae: 2.3038 - val_loss: 22.3442 - val_mse: 22.3442 - val_mae: 3.2625\n",
       "Epoch 20/75\n",
-      "404/404 [==============================] - 0s 401us/sample - loss: 10.2546 - mse: 10.2546 - mae: 2.2813 - val_loss: 26.5278 - val_mse: 26.5278 - val_mae: 3.4465\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 10.5816 - mse: 10.5816 - mae: 2.3382 - val_loss: 20.5198 - val_mse: 20.5198 - val_mae: 3.0412\n",
       "Epoch 21/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 10.1321 - mse: 10.1321 - mae: 2.2866 - val_loss: 24.0363 - val_mse: 24.0363 - val_mae: 3.2792\n",
+      "404/404 [==============================] - 0s 179us/sample - loss: 10.0658 - mse: 10.0658 - mae: 2.2640 - val_loss: 21.6221 - val_mse: 21.6221 - val_mae: 3.1056\n",
       "Epoch 22/75\n",
-      "404/404 [==============================] - 0s 421us/sample - loss: 9.9169 - mse: 9.9169 - mae: 2.2907 - val_loss: 23.7310 - val_mse: 23.7310 - val_mae: 3.2334\n",
+      "404/404 [==============================] - 0s 173us/sample - loss: 9.9239 - mse: 9.9239 - mae: 2.2311 - val_loss: 21.0508 - val_mse: 21.0508 - val_mae: 3.0422\n",
       "Epoch 23/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.6588 - mse: 9.6588 - mae: 2.2284 - val_loss: 23.6472 - val_mse: 23.6472 - val_mae: 3.2013\n",
+      "404/404 [==============================] - 0s 184us/sample - loss: 9.7311 - mse: 9.7311 - mae: 2.2363 - val_loss: 21.0378 - val_mse: 21.0378 - val_mae: 3.0248\n",
       "Epoch 24/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 9.6887 - mse: 9.6887 - mae: 2.2468 - val_loss: 23.5379 - val_mse: 23.5379 - val_mae: 3.1921\n",
+      "404/404 [==============================] - 0s 175us/sample - loss: 9.6331 - mse: 9.6331 - mae: 2.2317 - val_loss: 19.9315 - val_mse: 19.9315 - val_mae: 2.9388\n",
       "Epoch 25/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 9.4049 - mse: 9.4049 - mae: 2.1999 - val_loss: 23.7713 - val_mse: 23.7713 - val_mae: 3.2273\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 9.6127 - mse: 9.6127 - mae: 2.2050 - val_loss: 22.1209 - val_mse: 22.1209 - val_mae: 3.0676\n",
       "Epoch 26/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 9.2304 - mse: 9.2304 - mae: 2.1946 - val_loss: 23.5093 - val_mse: 23.5093 - val_mae: 3.2072\n",
+      "404/404 [==============================] - 0s 178us/sample - loss: 9.3259 - mse: 9.3259 - mae: 2.1685 - val_loss: 22.2937 - val_mse: 22.2937 - val_mae: 3.1008\n",
       "Epoch 27/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.0493 - mse: 9.0493 - mae: 2.1528 - val_loss: 23.7969 - val_mse: 23.7969 - val_mae: 3.2005\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 9.1805 - mse: 9.1805 - mae: 2.1410 - val_loss: 21.6367 - val_mse: 21.6367 - val_mae: 3.0285\n",
       "Epoch 28/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 8.9363 - mse: 8.9363 - mae: 2.1475 - val_loss: 22.1030 - val_mse: 22.1030 - val_mae: 3.0707\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 8.9269 - mse: 8.9269 - mae: 2.1561 - val_loss: 19.9453 - val_mse: 19.9453 - val_mae: 2.9300\n",
       "Epoch 29/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 8.7834 - mse: 8.7834 - mae: 2.1231 - val_loss: 22.5153 - val_mse: 22.5153 - val_mae: 3.1532\n",
+      "404/404 [==============================] - 0s 176us/sample - loss: 9.0746 - mse: 9.0746 - mae: 2.1111 - val_loss: 21.3031 - val_mse: 21.3031 - val_mae: 2.9900\n",
       "Epoch 30/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.7925 - mse: 8.7925 - mae: 2.1531 - val_loss: 22.0449 - val_mse: 22.0449 - val_mae: 3.1245\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 8.9010 - mse: 8.9010 - mae: 2.0904 - val_loss: 22.5624 - val_mse: 22.5624 - val_mae: 3.1063\n",
       "Epoch 31/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 9.1879 - mse: 9.1879 - mae: 2.2029 - val_loss: 22.1780 - val_mse: 22.1780 - val_mae: 3.0623\n",
+      "404/404 [==============================] - 0s 179us/sample - loss: 8.7388 - mse: 8.7388 - mae: 2.1280 - val_loss: 20.4689 - val_mse: 20.4689 - val_mae: 2.9479\n",
       "Epoch 32/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.7136 - mse: 8.7136 - mae: 2.1164 - val_loss: 21.9815 - val_mse: 21.9815 - val_mae: 3.0969\n",
+      "404/404 [==============================] - 0s 173us/sample - loss: 8.2990 - mse: 8.2990 - mae: 2.0350 - val_loss: 21.2429 - val_mse: 21.2429 - val_mae: 2.9529\n",
       "Epoch 33/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.3018 - mse: 8.3018 - mae: 2.0639 - val_loss: 21.0477 - val_mse: 21.0477 - val_mae: 2.9645\n",
+      "404/404 [==============================] - 0s 172us/sample - loss: 8.4887 - mse: 8.4887 - mae: 2.0662 - val_loss: 21.0671 - val_mse: 21.0671 - val_mae: 2.9332\n",
       "Epoch 34/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 8.4156 - mse: 8.4156 - mae: 2.0970 - val_loss: 22.6659 - val_mse: 22.6659 - val_mae: 3.1235\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 8.3696 - mse: 8.3696 - mae: 2.0589 - val_loss: 21.0642 - val_mse: 21.0642 - val_mae: 2.9676\n",
       "Epoch 35/75\n",
-      "404/404 [==============================] - 0s 350us/sample - loss: 8.2938 - mse: 8.2938 - mae: 2.0567 - val_loss: 20.9574 - val_mse: 20.9574 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 177us/sample - loss: 8.3205 - mse: 8.3205 - mae: 2.0490 - val_loss: 20.3748 - val_mse: 20.3748 - val_mae: 2.8913\n",
       "Epoch 36/75\n",
-      "404/404 [==============================] - 0s 357us/sample - loss: 8.0515 - mse: 8.0515 - mae: 2.0591 - val_loss: 23.2063 - val_mse: 23.2063 - val_mae: 3.1980\n",
+      "404/404 [==============================] - 0s 182us/sample - loss: 8.2199 - mse: 8.2199 - mae: 2.0660 - val_loss: 20.7762 - val_mse: 20.7762 - val_mae: 2.8651\n",
       "Epoch 37/75\n",
-      "404/404 [==============================] - 0s 381us/sample - loss: 8.1403 - mse: 8.1403 - mae: 2.0584 - val_loss: 24.5238 - val_mse: 24.5237 - val_mae: 3.3531\n",
+      "404/404 [==============================] - 0s 253us/sample - loss: 7.8603 - mse: 7.8603 - mae: 1.9902 - val_loss: 21.0590 - val_mse: 21.0590 - val_mae: 2.9437\n",
       "Epoch 38/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 8.0043 - mse: 8.0043 - mae: 2.0776 - val_loss: 22.5424 - val_mse: 22.5424 - val_mae: 3.1494\n",
+      "404/404 [==============================] - 0s 237us/sample - loss: 7.9022 - mse: 7.9022 - mae: 2.0366 - val_loss: 19.5327 - val_mse: 19.5327 - val_mae: 2.8398\n",
       "Epoch 39/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.1182 - mse: 8.1182 - mae: 2.0683 - val_loss: 19.7576 - val_mse: 19.7576 - val_mae: 2.8799\n",
+      "404/404 [==============================] - 0s 268us/sample - loss: 8.1130 - mse: 8.1130 - mae: 2.0066 - val_loss: 19.4996 - val_mse: 19.4996 - val_mae: 2.8398\n",
       "Epoch 40/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 7.8578 - mse: 7.8578 - mae: 2.0131 - val_loss: 20.7728 - val_mse: 20.7728 - val_mae: 2.9499\n",
+      "404/404 [==============================] - 0s 215us/sample - loss: 7.7803 - mse: 7.7803 - mae: 1.9898 - val_loss: 20.7776 - val_mse: 20.7776 - val_mae: 2.9260\n",
       "Epoch 41/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 7.5711 - mse: 7.5711 - mae: 1.9896 - val_loss: 20.6170 - val_mse: 20.6170 - val_mae: 2.9936\n",
+      "404/404 [==============================] - 0s 217us/sample - loss: 7.7724 - mse: 7.7724 - mae: 1.9694 - val_loss: 20.2665 - val_mse: 20.2665 - val_mae: 2.8344\n",
       "Epoch 42/75\n",
-      "404/404 [==============================] - 0s 385us/sample - loss: 7.5822 - mse: 7.5822 - mae: 1.9683 - val_loss: 20.8541 - val_mse: 20.8541 - val_mae: 3.0054\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 7.7714 - mse: 7.7714 - mae: 1.9781 - val_loss: 18.8184 - val_mse: 18.8184 - val_mae: 2.7368\n",
       "Epoch 43/75\n",
-      "404/404 [==============================] - 0s 408us/sample - loss: 7.4533 - mse: 7.4533 - mae: 1.9645 - val_loss: 20.4473 - val_mse: 20.4473 - val_mae: 2.8861\n",
+      "404/404 [==============================] - 0s 207us/sample - loss: 7.4646 - mse: 7.4646 - mae: 1.9634 - val_loss: 20.5761 - val_mse: 20.5760 - val_mae: 2.9118\n",
       "Epoch 44/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 7.5226 - mse: 7.5226 - mae: 1.9509 - val_loss: 20.5193 - val_mse: 20.5193 - val_mae: 2.9619\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 7.7174 - mse: 7.7174 - mae: 1.9938 - val_loss: 20.9050 - val_mse: 20.9050 - val_mae: 2.9116\n",
       "Epoch 45/75\n",
-      "404/404 [==============================] - 0s 355us/sample - loss: 7.2819 - mse: 7.2819 - mae: 1.9350 - val_loss: 21.4862 - val_mse: 21.4862 - val_mae: 2.9908\n",
+      "404/404 [==============================] - 0s 184us/sample - loss: 7.5331 - mse: 7.5331 - mae: 1.9487 - val_loss: 19.9968 - val_mse: 19.9968 - val_mae: 2.8637\n",
       "Epoch 46/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 7.0130 - mse: 7.0130 - mae: 1.9152 - val_loss: 20.1577 - val_mse: 20.1577 - val_mae: 2.9370\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 7.1290 - mse: 7.1290 - mae: 1.9217 - val_loss: 19.1801 - val_mse: 19.1801 - val_mae: 2.8253\n",
       "Epoch 47/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.9431 - mse: 6.9431 - mae: 1.8819 - val_loss: 21.1210 - val_mse: 21.1210 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 7.3044 - mse: 7.3044 - mae: 1.9145 - val_loss: 18.9886 - val_mse: 18.9886 - val_mae: 2.7550\n",
       "Epoch 48/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 6.8982 - mse: 6.8982 - mae: 1.9037 - val_loss: 19.2999 - val_mse: 19.2999 - val_mae: 2.8638\n",
+      "404/404 [==============================] - 0s 174us/sample - loss: 7.2084 - mse: 7.2084 - mae: 1.8912 - val_loss: 18.1262 - val_mse: 18.1262 - val_mae: 2.6925\n",
       "Epoch 49/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 6.9521 - mse: 6.9521 - mae: 1.8862 - val_loss: 20.7825 - val_mse: 20.7825 - val_mae: 2.9369\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 7.0196 - mse: 7.0196 - mae: 1.8731 - val_loss: 18.7925 - val_mse: 18.7925 - val_mae: 2.6957\n",
       "Epoch 50/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.8718 - mse: 6.8718 - mae: 1.8889 - val_loss: 20.0288 - val_mse: 20.0288 - val_mae: 2.8915\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 6.9054 - mse: 6.9054 - mae: 1.8941 - val_loss: 19.9679 - val_mse: 19.9679 - val_mae: 2.7829\n",
       "Epoch 51/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 6.7111 - mse: 6.7111 - mae: 1.8702 - val_loss: 20.4913 - val_mse: 20.4913 - val_mae: 3.0116\n",
+      "404/404 [==============================] - 0s 175us/sample - loss: 6.9380 - mse: 6.9380 - mae: 1.8531 - val_loss: 20.4492 - val_mse: 20.4492 - val_mae: 2.9579\n",
       "Epoch 52/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 6.7492 - mse: 6.7492 - mae: 1.8482 - val_loss: 18.3008 - val_mse: 18.3008 - val_mae: 2.7362\n",
+      "404/404 [==============================] - 0s 176us/sample - loss: 7.0332 - mse: 7.0332 - mae: 1.8935 - val_loss: 19.3400 - val_mse: 19.3400 - val_mae: 2.7723\n",
       "Epoch 53/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.6262 - mse: 6.6262 - mae: 1.8395 - val_loss: 18.1885 - val_mse: 18.1885 - val_mae: 2.6920\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 6.8921 - mse: 6.8921 - mae: 1.8694 - val_loss: 18.6543 - val_mse: 18.6543 - val_mae: 2.7821\n",
       "Epoch 54/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 6.7148 - mse: 6.7148 - mae: 1.8611 - val_loss: 18.5764 - val_mse: 18.5764 - val_mae: 2.6977\n",
+      "404/404 [==============================] - 0s 221us/sample - loss: 7.0062 - mse: 7.0062 - mae: 1.9165 - val_loss: 19.1606 - val_mse: 19.1606 - val_mae: 2.7402\n",
       "Epoch 55/75\n",
-      "404/404 [==============================] - 0s 358us/sample - loss: 6.5425 - mse: 6.5425 - mae: 1.8522 - val_loss: 19.5772 - val_mse: 19.5772 - val_mae: 2.8326\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 6.7402 - mse: 6.7402 - mae: 1.8432 - val_loss: 20.3731 - val_mse: 20.3731 - val_mae: 2.8229\n",
       "Epoch 56/75\n",
-      "404/404 [==============================] - 0s 423us/sample - loss: 6.3349 - mse: 6.3349 - mae: 1.8175 - val_loss: 19.0932 - val_mse: 19.0932 - val_mae: 2.8260\n",
+      "404/404 [==============================] - 0s 218us/sample - loss: 6.6122 - mse: 6.6122 - mae: 1.8171 - val_loss: 19.3740 - val_mse: 19.3740 - val_mae: 2.7955\n",
       "Epoch 57/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.4253 - mse: 6.4253 - mae: 1.7972 - val_loss: 20.4036 - val_mse: 20.4036 - val_mae: 2.9258\n",
+      "404/404 [==============================] - 0s 178us/sample - loss: 6.4625 - mse: 6.4625 - mae: 1.8247 - val_loss: 18.2020 - val_mse: 18.2020 - val_mae: 2.7036\n",
       "Epoch 58/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.2897 - mse: 6.2897 - mae: 1.7785 - val_loss: 21.2845 - val_mse: 21.2845 - val_mae: 3.0715\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 6.5036 - mse: 6.5036 - mae: 1.8027 - val_loss: 18.9655 - val_mse: 18.9655 - val_mae: 2.6959\n",
       "Epoch 59/75\n",
-      "404/404 [==============================] - 0s 378us/sample - loss: 6.7839 - mse: 6.7839 - mae: 1.9027 - val_loss: 18.6853 - val_mse: 18.6853 - val_mae: 2.7709\n",
+      "404/404 [==============================] - 0s 190us/sample - loss: 6.4785 - mse: 6.4785 - mae: 1.8061 - val_loss: 18.8109 - val_mse: 18.8109 - val_mae: 2.7700\n",
       "Epoch 60/75\n",
-      "404/404 [==============================] - 0s 395us/sample - loss: 6.7178 - mse: 6.7178 - mae: 1.8871 - val_loss: 19.5394 - val_mse: 19.5394 - val_mae: 2.8101\n",
+      "404/404 [==============================] - 0s 215us/sample - loss: 6.2863 - mse: 6.2863 - mae: 1.7910 - val_loss: 19.8210 - val_mse: 19.8210 - val_mae: 2.8430\n",
       "Epoch 61/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 6.4152 - mse: 6.4152 - mae: 1.8175 - val_loss: 18.2377 - val_mse: 18.2377 - val_mae: 2.7450\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 6.1627 - mse: 6.1627 - mae: 1.7516 - val_loss: 18.6837 - val_mse: 18.6838 - val_mae: 2.6763\n",
       "Epoch 62/75\n",
-      "404/404 [==============================] - 0s 384us/sample - loss: 5.9727 - mse: 5.9727 - mae: 1.7630 - val_loss: 19.0252 - val_mse: 19.0252 - val_mae: 2.7960\n",
+      "404/404 [==============================] - 0s 206us/sample - loss: 6.2671 - mse: 6.2671 - mae: 1.7886 - val_loss: 17.7927 - val_mse: 17.7927 - val_mae: 2.6681\n",
       "Epoch 63/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 6.0973 - mse: 6.0973 - mae: 1.8071 - val_loss: 18.8069 - val_mse: 18.8069 - val_mae: 2.8894\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 6.2252 - mse: 6.2252 - mae: 1.7666 - val_loss: 17.2918 - val_mse: 17.2918 - val_mae: 2.6189\n",
       "Epoch 64/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.1074 - mse: 6.1074 - mae: 1.7978 - val_loss: 18.4702 - val_mse: 18.4702 - val_mae: 2.7851\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 6.1191 - mse: 6.1191 - mae: 1.7307 - val_loss: 19.2678 - val_mse: 19.2678 - val_mae: 2.7358\n",
       "Epoch 65/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 5.9329 - mse: 5.9329 - mae: 1.7545 - val_loss: 18.5321 - val_mse: 18.5321 - val_mae: 2.7933\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 6.2865 - mse: 6.2865 - mae: 1.7720 - val_loss: 18.7232 - val_mse: 18.7232 - val_mae: 2.7234\n",
       "Epoch 66/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.7473 - mse: 5.7473 - mae: 1.7211 - val_loss: 18.5536 - val_mse: 18.5536 - val_mae: 2.8010\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 5.9686 - mse: 5.9686 - mae: 1.7074 - val_loss: 17.9657 - val_mse: 17.9657 - val_mae: 2.6617\n",
       "Epoch 67/75\n",
-      "404/404 [==============================] - 0s 339us/sample - loss: 5.8866 - mse: 5.8866 - mae: 1.7224 - val_loss: 18.0067 - val_mse: 18.0067 - val_mae: 2.7054\n",
+      "404/404 [==============================] - 0s 198us/sample - loss: 5.9193 - mse: 5.9193 - mae: 1.7102 - val_loss: 18.5935 - val_mse: 18.5935 - val_mae: 2.7495\n",
       "Epoch 68/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.7885 - mse: 5.7885 - mae: 1.7391 - val_loss: 17.5502 - val_mse: 17.5502 - val_mae: 2.6767\n",
+      "404/404 [==============================] - 0s 218us/sample - loss: 5.9622 - mse: 5.9622 - mae: 1.7648 - val_loss: 17.2383 - val_mse: 17.2383 - val_mae: 2.6095\n",
       "Epoch 69/75\n",
-      "404/404 [==============================] - 0s 331us/sample - loss: 5.8809 - mse: 5.8809 - mae: 1.7542 - val_loss: 17.0280 - val_mse: 17.0280 - val_mae: 2.6404\n",
+      "404/404 [==============================] - 0s 195us/sample - loss: 5.7503 - mse: 5.7503 - mae: 1.7027 - val_loss: 17.6104 - val_mse: 17.6104 - val_mae: 2.6336\n",
       "Epoch 70/75\n",
-      "404/404 [==============================] - 0s 343us/sample - loss: 5.6028 - mse: 5.6028 - mae: 1.6972 - val_loss: 17.7188 - val_mse: 17.7188 - val_mae: 2.6979\n",
+      "404/404 [==============================] - 0s 211us/sample - loss: 5.6720 - mse: 5.6720 - mae: 1.6820 - val_loss: 18.4748 - val_mse: 18.4748 - val_mae: 2.6705\n",
       "Epoch 71/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.4361 - mse: 5.4361 - mae: 1.6741 - val_loss: 16.8852 - val_mse: 16.8852 - val_mae: 2.6126\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 5.6052 - mse: 5.6052 - mae: 1.7128 - val_loss: 17.8656 - val_mse: 17.8656 - val_mae: 2.6559\n",
       "Epoch 72/75\n",
-      "404/404 [==============================] - 0s 345us/sample - loss: 5.5608 - mse: 5.5608 - mae: 1.7252 - val_loss: 16.7483 - val_mse: 16.7483 - val_mae: 2.6063\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 5.6627 - mse: 5.6627 - mae: 1.7121 - val_loss: 18.6519 - val_mse: 18.6519 - val_mae: 2.7489\n",
       "Epoch 73/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.5022 - mse: 5.5022 - mae: 1.6912 - val_loss: 17.6786 - val_mse: 17.6786 - val_mae: 2.7316\n",
+      "404/404 [==============================] - 0s 184us/sample - loss: 5.5228 - mse: 5.5228 - mae: 1.6610 - val_loss: 17.7915 - val_mse: 17.7915 - val_mae: 2.6664\n",
       "Epoch 74/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 5.2794 - mse: 5.2794 - mae: 1.6478 - val_loss: 17.6115 - val_mse: 17.6115 - val_mae: 2.6773\n",
+      "404/404 [==============================] - 0s 178us/sample - loss: 5.4312 - mse: 5.4312 - mae: 1.6560 - val_loss: 17.8674 - val_mse: 17.8674 - val_mae: 2.6744\n",
       "Epoch 75/75\n",
-      "404/404 [==============================] - 0s 338us/sample - loss: 5.4796 - mse: 5.4796 - mae: 1.6876 - val_loss: 17.2835 - val_mse: 17.2835 - val_mae: 2.7126\n"
+      "404/404 [==============================] - 0s 191us/sample - loss: 5.5085 - mse: 5.5085 - mae: 1.6893 - val_loss: 18.0374 - val_mse: 18.0374 - val_mae: 2.7064\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f36340c6b38>"
+       "<tensorflow.python.keras.callbacks.History at 0x1440c0e80>"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -417,7 +413,6 @@
     "## What Hyperparameters are there to test?\n",
     "\n",
     "- batch_size\n",
-    "- training epochs\n",
     "- optimization algorithms\n",
     "- learning rate\n",
     "- momentum\n",
@@ -449,7 +444,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 9,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -460,25 +455,17 @@
     "outputId": "ae996575-78e2-43fb-9dbe-5d44aaf0b430"
    },
    "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
-      "  warnings.warn(CV_WARNING, FutureWarning)\n"
-     ]
-    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Best: 0.65234375 using {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.65234375, Stdev: 0.033298728782667764 with: {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6263020833333334, Stdev: 0.01813592223591682 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6041666666666666, Stdev: 0.037782859709757574 with: {'batch_size': 40, 'epochs': 20}\n",
-      "Means: 0.5533854166666666, Stdev: 0.03210632293213009 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.61328125, Stdev: 0.024079742199097563 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.5611979166666666, Stdev: 0.038450060052691144 with: {'batch_size': 100, 'epochs': 20}\n"
+      "Best: 0.6550632357597351 using {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6326627731323242, Stdev: 0.053739447760642364 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6550632357597351, Stdev: 0.03873597023264761 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6042356491088867, Stdev: 0.033962453537288856 with: {'batch_size': 40, 'epochs': 20}\n",
+      "Means: 0.6262965798377991, Stdev: 0.02364799838304432 with: {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.5924199938774108, Stdev: 0.023305833006304417 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.5298192083835602, Stdev: 0.06749446690447318 with: {'batch_size': 100, 'epochs': 20}\n"
      ]
     }
    ],
@@ -537,61 +524,6 @@
     "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {
-    "colab_type": "text",
-    "id": "pmABfjlvXbqi"
-   },
-   "source": [
-    "## Epochs\n",
-    "\n",
-    "The number of training epochs has a large and direct affect on the accuracy, However, more epochs is almost always goign to better than less epochs. This means that if you tune this parameter at the beginning and try and maintain the same value all throughout your training, you're going to be waiting a long time for each iteration of GridSearch. I suggest picking a fixed moderat # of epochs all throughout your training and then Grid Searching this parameter at the very end. "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {
-    "colab": {
-     "base_uri": "https://localhost:8080/",
-     "height": 26329
-    },
-    "colab_type": "code",
-    "id": "bAmxP3N7TmFh",
-    "outputId": "3ddb08c4-51ac-4eaa-ff39-143397024544"
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Best: 0.7044270833333334 using {'batch_size': 20, 'epochs': 200}\n",
-      "Means: 0.6666666666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6588541666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 40}\n",
-      "Means: 0.6848958333333334, Stdev: 0.03498705427745938 with: {'batch_size': 20, 'epochs': 60}\n",
-      "Means: 0.7044270833333334, Stdev: 0.018414239093399672 with: {'batch_size': 20, 'epochs': 200}\n"
-     ]
-    }
-   ],
-   "source": [
-    "# define the grid search parameters\n",
-    "param_grid = {'batch_size': [20],\n",
-    "              'epochs': [20, 40, 60,200]}\n",
-    "\n",
-    "# Create Grid Search\n",
-    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
-    "grid_result = grid.fit(X, Y)\n",
-    "\n",
-    "# Report Results\n",
-    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
-    "means = grid_result.cv_results_['mean_test_score']\n",
-    "stds = grid_result.cv_results_['std_test_score']\n",
-    "params = grid_result.cv_results_['params']\n",
-    "for mean, stdev, param in zip(means, stds, params):\n",
-    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -604,6 +536,13 @@
     "Remember that there's a different optimizers [optimizers](https://keras.io/optimizers/). At some point, take some time to read up on them a little bit. \"adam\" usually gives the best results. The thing to know about choosing an optimizer is that different optimizers have different hyperparameters like learning rate, momentum, etc. So based on the optimizer you choose you might also have to tune the learning rate and momentum of those optimizers after that. "
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -730,44 +669,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "W&B Run: https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro"
-      ]
-     },
-     "execution_count": 6,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "import wandb\n",
-    "from wandb.keras import WandbCallback"
+    "from wandb.keras import WandbCallback\n",
+    "from tensorflow.keras.optimizers import Adam"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": null,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -777,143 +690,10 @@
     "id": "GMXVfmzXp1Oo",
     "outputId": "b05e251e-508f-46e6-865b-f869ae2a5dc4"
    },
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Train on 270 samples, validate on 134 samples\n",
-      "Epoch 1/50\n",
-      "270/270 [==============================] - 1s 3ms/sample - loss: 492.3539 - mse: 492.3539 - mae: 20.3197 - val_loss: 481.5445 - val_mse: 481.5445 - val_mae: 19.6138\n",
-      "Epoch 2/50\n",
-      "270/270 [==============================] - 0s 591us/sample - loss: 239.4999 - mse: 239.4999 - mae: 12.8064 - val_loss: 113.8561 - val_mse: 113.8561 - val_mae: 8.2962\n",
-      "Epoch 3/50\n",
-      "270/270 [==============================] - 0s 618us/sample - loss: 56.2921 - mse: 56.2921 - mae: 5.8988 - val_loss: 62.7912 - val_mse: 62.7912 - val_mae: 5.6465\n",
-      "Epoch 4/50\n",
-      "270/270 [==============================] - 0s 613us/sample - loss: 29.4994 - mse: 29.4994 - mae: 3.9653 - val_loss: 37.9256 - val_mse: 37.9256 - val_mae: 4.1730\n",
-      "Epoch 5/50\n",
-      "270/270 [==============================] - 0s 608us/sample - loss: 20.6919 - mse: 20.6919 - mae: 3.3022 - val_loss: 31.7489 - val_mse: 31.7489 - val_mae: 3.7113\n",
-      "Epoch 6/50\n",
-      "270/270 [==============================] - 0s 602us/sample - loss: 17.2701 - mse: 17.2701 - mae: 3.0291 - val_loss: 27.3921 - val_mse: 27.3921 - val_mae: 3.4958\n",
-      "Epoch 7/50\n",
-      "270/270 [==============================] - 0s 671us/sample - loss: 15.5172 - mse: 15.5172 - mae: 2.8537 - val_loss: 25.3208 - val_mse: 25.3208 - val_mae: 3.3650\n",
-      "Epoch 8/50\n",
-      "270/270 [==============================] - 0s 661us/sample - loss: 13.7548 - mse: 13.7548 - mae: 2.7089 - val_loss: 23.8920 - val_mse: 23.8920 - val_mae: 3.2746\n",
-      "Epoch 9/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 12.3745 - mse: 12.3745 - mae: 2.5662 - val_loss: 22.1294 - val_mse: 22.1294 - val_mae: 3.1509\n",
-      "Epoch 10/50\n",
-      "270/270 [==============================] - 0s 614us/sample - loss: 11.2424 - mse: 11.2424 - mae: 2.4804 - val_loss: 20.5718 - val_mse: 20.5718 - val_mae: 3.0461\n",
-      "Epoch 11/50\n",
-      "270/270 [==============================] - 0s 605us/sample - loss: 10.6098 - mse: 10.6098 - mae: 2.4178 - val_loss: 20.3467 - val_mse: 20.3467 - val_mae: 3.0251\n",
-      "Epoch 12/50\n",
-      "270/270 [==============================] - 0s 576us/sample - loss: 10.0011 - mse: 10.0011 - mae: 2.3257 - val_loss: 18.4283 - val_mse: 18.4283 - val_mae: 2.8938\n",
-      "Epoch 13/50\n",
-      "270/270 [==============================] - 0s 666us/sample - loss: 9.1287 - mse: 9.1287 - mae: 2.2384 - val_loss: 18.2024 - val_mse: 18.2024 - val_mae: 2.9116\n",
-      "Epoch 14/50\n",
-      "270/270 [==============================] - 0s 603us/sample - loss: 8.6211 - mse: 8.6211 - mae: 2.1980 - val_loss: 17.4749 - val_mse: 17.4749 - val_mae: 2.8290\n",
-      "Epoch 15/50\n",
-      "270/270 [==============================] - 0s 463us/sample - loss: 8.4558 - mse: 8.4558 - mae: 2.2087 - val_loss: 17.7878 - val_mse: 17.7878 - val_mae: 2.8516\n",
-      "Epoch 16/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 8.3626 - mse: 8.3626 - mae: 2.2031 - val_loss: 16.7101 - val_mse: 16.7101 - val_mae: 2.7820\n",
-      "Epoch 17/50\n",
-      "270/270 [==============================] - 0s 607us/sample - loss: 7.9180 - mse: 7.9180 - mae: 2.1265 - val_loss: 16.6064 - val_mse: 16.6064 - val_mae: 2.7419\n",
-      "Epoch 18/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 7.5552 - mse: 7.5552 - mae: 2.0235 - val_loss: 17.2872 - val_mse: 17.2872 - val_mae: 2.8539\n",
-      "Epoch 19/50\n",
-      "270/270 [==============================] - 0s 616us/sample - loss: 7.0971 - mse: 7.0971 - mae: 2.0038 - val_loss: 16.5110 - val_mse: 16.5110 - val_mae: 2.8042\n",
-      "Epoch 20/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 6.7068 - mse: 6.7068 - mae: 1.9539 - val_loss: 15.5886 - val_mse: 15.5886 - val_mae: 2.7048\n",
-      "Epoch 21/50\n",
-      "270/270 [==============================] - 0s 461us/sample - loss: 6.8542 - mse: 6.8542 - mae: 1.9979 - val_loss: 17.2378 - val_mse: 17.2378 - val_mae: 2.8853\n",
-      "Epoch 22/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 6.5719 - mse: 6.5719 - mae: 1.9312 - val_loss: 16.3043 - val_mse: 16.3043 - val_mae: 2.7756\n",
-      "Epoch 23/50\n",
-      "270/270 [==============================] - 0s 478us/sample - loss: 6.6161 - mse: 6.6161 - mae: 1.9572 - val_loss: 15.7992 - val_mse: 15.7992 - val_mae: 2.7219\n",
-      "Epoch 24/50\n",
-      "270/270 [==============================] - 0s 491us/sample - loss: 7.1269 - mse: 7.1269 - mae: 2.0137 - val_loss: 16.5402 - val_mse: 16.5402 - val_mae: 2.8005\n",
-      "Epoch 25/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 6.3382 - mse: 6.3382 - mae: 1.8540 - val_loss: 16.5034 - val_mse: 16.5034 - val_mae: 2.7864\n",
-      "Epoch 26/50\n",
-      "270/270 [==============================] - 0s 488us/sample - loss: 5.9442 - mse: 5.9442 - mae: 1.8251 - val_loss: 15.6558 - val_mse: 15.6558 - val_mae: 2.7102\n",
-      "Epoch 27/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 5.5832 - mse: 5.5832 - mae: 1.7432 - val_loss: 15.3021 - val_mse: 15.3021 - val_mae: 2.6862\n",
-      "Epoch 28/50\n",
-      "270/270 [==============================] - 0s 436us/sample - loss: 5.4530 - mse: 5.4530 - mae: 1.7354 - val_loss: 15.4570 - val_mse: 15.4570 - val_mae: 2.6846\n",
-      "Epoch 29/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 5.3070 - mse: 5.3070 - mae: 1.7079 - val_loss: 15.8510 - val_mse: 15.8510 - val_mae: 2.7644\n",
-      "Epoch 30/50\n",
-      "270/270 [==============================] - 0s 477us/sample - loss: 5.4157 - mse: 5.4157 - mae: 1.7321 - val_loss: 15.9160 - val_mse: 15.9160 - val_mae: 2.7134\n",
-      "Epoch 31/50\n",
-      "270/270 [==============================] - 0s 452us/sample - loss: 5.2639 - mse: 5.2639 - mae: 1.6981 - val_loss: 15.3554 - val_mse: 15.3554 - val_mae: 2.6662\n",
-      "Epoch 32/50\n",
-      "270/270 [==============================] - 0s 475us/sample - loss: 5.7687 - mse: 5.7687 - mae: 1.8045 - val_loss: 15.7151 - val_mse: 15.7151 - val_mae: 2.6867\n",
-      "Epoch 33/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 5.5210 - mse: 5.5210 - mae: 1.7367 - val_loss: 15.4227 - val_mse: 15.4227 - val_mae: 2.6561\n",
-      "Epoch 34/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 5.5663 - mse: 5.5663 - mae: 1.7294 - val_loss: 15.3376 - val_mse: 15.3376 - val_mae: 2.6991\n",
-      "Epoch 35/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 5.0063 - mse: 5.0063 - mae: 1.6196 - val_loss: 15.2642 - val_mse: 15.2642 - val_mae: 2.6796\n",
-      "Epoch 36/50\n",
-      "270/270 [==============================] - 0s 459us/sample - loss: 4.7251 - mse: 4.7251 - mae: 1.5727 - val_loss: 15.4858 - val_mse: 15.4858 - val_mae: 2.7288\n",
-      "Epoch 37/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 4.6394 - mse: 4.6394 - mae: 1.5854 - val_loss: 15.1139 - val_mse: 15.1139 - val_mae: 2.6305\n",
-      "Epoch 38/50\n",
-      "270/270 [==============================] - 0s 592us/sample - loss: 4.5669 - mse: 4.5669 - mae: 1.5548 - val_loss: 14.9898 - val_mse: 14.9898 - val_mae: 2.6340\n",
-      "Epoch 39/50\n",
-      "270/270 [==============================] - 0s 458us/sample - loss: 4.4480 - mse: 4.4480 - mae: 1.5334 - val_loss: 15.6389 - val_mse: 15.6389 - val_mae: 2.7337\n",
-      "Epoch 40/50\n",
-      "270/270 [==============================] - 0s 455us/sample - loss: 4.4119 - mse: 4.4119 - mae: 1.5426 - val_loss: 15.0723 - val_mse: 15.0723 - val_mae: 2.6709\n",
-      "Epoch 41/50\n",
-      "270/270 [==============================] - 0s 473us/sample - loss: 4.0797 - mse: 4.0797 - mae: 1.4725 - val_loss: 15.4706 - val_mse: 15.4706 - val_mae: 2.6707\n",
-      "Epoch 42/50\n",
-      "270/270 [==============================] - 0s 449us/sample - loss: 4.0619 - mse: 4.0619 - mae: 1.4692 - val_loss: 15.2423 - val_mse: 15.2423 - val_mae: 2.6165\n",
-      "Epoch 43/50\n",
-      "270/270 [==============================] - 0s 465us/sample - loss: 4.1861 - mse: 4.1861 - mae: 1.5076 - val_loss: 15.7510 - val_mse: 15.7510 - val_mae: 2.7279\n",
-      "Epoch 44/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 4.1128 - mse: 4.1128 - mae: 1.4810 - val_loss: 15.4814 - val_mse: 15.4814 - val_mae: 2.6562\n",
-      "Epoch 45/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 4.2171 - mse: 4.2171 - mae: 1.5205 - val_loss: 16.3839 - val_mse: 16.3839 - val_mae: 2.8194\n",
-      "Epoch 46/50\n",
-      "270/270 [==============================] - 0s 422us/sample - loss: 4.2609 - mse: 4.2609 - mae: 1.5548 - val_loss: 15.3587 - val_mse: 15.3587 - val_mae: 2.7161\n",
-      "Epoch 47/50\n",
-      "270/270 [==============================] - 0s 454us/sample - loss: 4.4635 - mse: 4.4635 - mae: 1.5440 - val_loss: 15.7736 - val_mse: 15.7736 - val_mae: 2.7184\n",
-      "Epoch 48/50\n",
-      "270/270 [==============================] - 0s 426us/sample - loss: 3.7406 - mse: 3.7406 - mae: 1.4147 - val_loss: 15.6718 - val_mse: 15.6718 - val_mae: 2.7468\n",
-      "Epoch 49/50\n",
-      "270/270 [==============================] - 0s 445us/sample - loss: 3.6173 - mse: 3.6173 - mae: 1.3816 - val_loss: 15.7291 - val_mse: 15.7291 - val_mae: 2.7789\n",
-      "Epoch 50/50\n",
-      "270/270 [==============================] - 0s 430us/sample - loss: 3.6303 - mse: 3.6303 - mae: 1.4266 - val_loss: 15.4937 - val_mse: 15.4937 - val_mae: 2.7390\n"
-     ]
-    },
-    {
-     "data": {
-      "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f315c319be0>"
-      ]
-     },
-     "execution_count": 8,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
-    "wandb.init(project=\"boston\", entity=\"lambda-ds7\") #Initializes and Experiment\n",
+    "#Initializes and Experiment\n",
+    "wandb.init(project=wandb_project, entity=wandb_group)\n",
     "\n",
     "# Important Hyperparameters\n",
     "X =  x_train\n",
@@ -930,7 +710,7 @@
     "model.add(Dense(64, activation='relu'))\n",
     "model.add(Dense(1))\n",
     "# Compile Model\n",
-    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
+    "model.compile(optimizer=Adam(learning_rate=0.02), loss='mse', metrics=['mse', 'mae'])\n",
     "\n",
     "# Fit Model\n",
     "model.fit(X, y, \n",
@@ -950,150 +730,6 @@
     "You will be expected to use Weights & Biases to try to tune your model during your module assignment today. "
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Hyperparameters with RandomSearchCV (Learn)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Overview\n",
-    "\n",
-    "Basically `GridSearchCV` takes forever. You'll want to adopt a slightly more sophiscated strategy."
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Follow Along"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "sweep_config = {\n",
-    "    'method': 'random',\n",
-    "    'parameters': {\n",
-    "        'learning_rate': {'distribution': 'normal'},\n",
-    "        'epochs': {'distribution': 'uniform',\n",
-    "                    'min': 100,\n",
-    "                    'max': 1000},\n",
-    "        'batch_size': {'distribution': 'uniform',\n",
-    "            'min': 10,\n",
-    "            'max': 400}\n",
-    "    }\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 13,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Create sweep with ID: huau0u9r\n",
-      "Sweep URL: https://app.wandb.ai/lambda-ds7/boston/sweeps/huau0u9r\n"
-     ]
-    }
-   ],
-   "source": [
-    "sweep_id = wandb.sweep(sweep_config)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 11,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import wandb\n",
-    "from wandb.keras import WandbCallback\n",
-    "#Initializes and Experiment\n",
-    "\n",
-    "from tensorflow.keras.optimizers import Adam\n",
-    "\n",
-    "# Important Hyperparameters\n",
-    "X =  x_train\n",
-    "y =  y_train\n",
-    "\n",
-    "inputs = X.shape[1]\n",
-    "\n",
-    "def train():\n",
-    "    \n",
-    "    wandb.init(project=\"boston\", entity=\"lambda-ds7\") \n",
-    "    \n",
-    "    config = wandb.config\n",
-    "\n",
-    "    # Create Model\n",
-    "    model = Sequential()\n",
-    "    model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
-    "    model.add(Dense(64, activation='relu'))\n",
-    "    model.add(Dense(64, activation='relu'))\n",
-    "    model.add(Dense(1))\n",
-    "\n",
-    "    # Optimizer \n",
-    "    adam = Adam(learning_rate=config.learning_rate)\n",
-    "\n",
-    "    # Compile Model\n",
-    "    model.compile(optimizer=adam, loss='mse', metrics=['mse', 'mae'])\n",
-    "\n",
-    "    # Fit Model\n",
-    "    model.fit(X, y, \n",
-    "              validation_split=0.33, \n",
-    "              epochs=config.epochs, \n",
-    "              batch_size=config.batch_size, \n",
-    "              callbacks=[WandbCallback()]\n",
-    "             )"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "wandb: Agent Starting Run: 2g77kp6k with config:\n",
-      "\tbatch_size: 308.503347845309\n",
-      "\tepochs: 704.9395850579006\n",
-      "\tlearning_rate: 1.480005523005428\n",
-      "wandb: Agent Started Run: 2g77kp6k\n"
-     ]
-    },
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "wandb.agent(sweep_id, function=train)"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
@@ -1122,10 +758,7 @@
     "    - Weights & Biases\n",
     "    - Comet.ml\n",
     "    - By Hand / GridSearch\n",
-    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch\n",
-    "    - Sklearn still useful (haha)\n",
-    "    - Integration with Wieghts & Biases\n",
-    "* <a href=\"#p4\">Part 4</a>: Discuss emerging hyperparameter tuning strategies\n",
+    "* <a href=\"#p3\">Optional</a>: Discuss emerging hyperparameter tuning strategies\n",
     "    - Bayesian Optimization\n",
     "    - Hyperopt\n",
     "    - Genetic Evolution"
@@ -1153,9 +786,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "conda_tensorflow_p36",
+   "display_name": "4-2",
    "language": "python",
-   "name": "conda_tensorflow_p36"
+   "name": "4-2"
   },
   "language_info": {
    "codemirror_mode": {
@@ -1167,7 +800,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.5"
+   "version": "3.7.0"
   }
  },
  "nbformat": 4,
