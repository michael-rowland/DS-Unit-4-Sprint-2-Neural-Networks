diff --git a/Warmups.ipynb b/Warmups.ipynb
index fbf1f01..0dccc35 100644
--- a/Warmups.ipynb
+++ b/Warmups.ipynb
@@ -1,8 +1,25 @@
 {
  "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Anika wants to run a Secret Santa for Christmas but has a lot of friends and doesn't want to have to assign everyone by hand. Write a function that takes in an ordered list of names, shuffles it, and then assigns each person a Secret Santa. Per the rules of Secret Santa, no one should be receiving from someone they gave to.\n",
+    "\n",
+    "```Example:\n",
+    "santas = ['a','b','c','d','e']\n",
+    "secret_santafy(santas) ->\n",
+    "c b\n",
+    "b e\n",
+    "e a\n",
+    "a d\n",
+    "d c\n",
+    "```"
+   ]
+  },
   {
    "cell_type": "code",
-   "execution_count": 41,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -11,7 +28,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 42,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -26,16 +43,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 43,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "KEITH CONNOR\n",
-      "CONNOR JANE\n",
-      "JANE JIMMY\n",
+      "KEITH JANE\n",
+      "JANE CONNOR\n",
+      "CONNOR JIMMY\n",
       "JIMMY MICHAEL\n",
       "MICHAEL KEITH\n"
      ]
@@ -45,6 +62,64 @@
     "names = ['KEITH', 'JANE', 'MICHAEL', 'JIMMY', 'CONNOR']\n",
     "santa(names)"
    ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Write a function that takes in a non-empty array of distinct integers and an integer representing a target sum. If any two numbers in the input array sum up to the target sum, the function should return them in an array, in sorted order. If no two numbers sum up to the target sum, the function should return an empty array.\n",
+    "Assume that there will be at most one pair of numbers summing up to the target sum.\n",
+    "\n",
+    "Sample input: [3,5,-4,8,11,1,-1,6], 10\n",
+    "\n",
+    "Sample output: [-1,11]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def sum_finder(input_list=None, sum_total=0):\n",
+    "    pairs = []\n",
+    "    for i in input_list:\n",
+    "        for j in input_list:\n",
+    "            if i + j == sum_total:\n",
+    "                print(i, j)\n",
+    "                pairs.append((i, j))\n",
+    "    \n",
+    "    return pairs"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "5 5\n",
+      "11 -1\n",
+      "-1 11\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[(5, 5), (11, -1), (-1, 11)]"
+      ]
+     },
+     "execution_count": 7,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "sum_finder([3,5,-4,8,11,1,-1,6], 10)"
+   ]
   }
  ],
  "metadata": {
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
index 9c59d8c..a00b42b 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
@@ -91,7 +91,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index 4bb18e9..dde6308 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -21,10 +21,7 @@
     "id": "41TS0Sa0rDNx"
    },
    "source": [
-    "# Neural Networks & GPUs (Prepare)\n",
-    "*aka Hyperparameter Tuning*\n",
-    "\n",
-    "*aka Big Servers for Big Problems*"
+    "# Neural Networks Hyperparameter Tuning (Prepare)"
    ]
   },
   {
@@ -36,8 +33,7 @@
    "source": [
     "## Learning Objectives\n",
     "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparemeters to tune\n",
-    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
-    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch (Optional)"
+    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework"
    ]
   },
   {
@@ -47,7 +43,7 @@
    "outputs": [],
    "source": [
     "wandb_group = \"ds8\"\n",
-    "wandb_project = \"inclass\""
+    "wandb_project = \"ds11_inclass\""
    ]
   },
   {
@@ -81,7 +77,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -106,7 +102,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
@@ -170,7 +166,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -187,164 +183,164 @@
      "text": [
       "Train on 404 samples, validate on 102 samples\n",
       "Epoch 1/75\n",
-      "404/404 [==============================] - 2s 4ms/sample - loss: 498.2045 - mse: 498.2046 - mae: 20.2543 - val_loss: 421.5039 - val_mse: 421.5038 - val_mae: 18.3349\n",
+      "404/404 [==============================] - 1s 1ms/sample - loss: 526.2315 - mse: 526.2314 - mae: 20.9827 - val_loss: 461.7852 - val_mse: 461.7852 - val_mae: 19.4971\n",
       "Epoch 2/75\n",
-      "404/404 [==============================] - 0s 347us/sample - loss: 249.6985 - mse: 249.6985 - mae: 13.2672 - val_loss: 111.3743 - val_mse: 111.3743 - val_mae: 8.6210\n",
+      "404/404 [==============================] - 0s 199us/sample - loss: 297.7244 - mse: 297.7245 - mae: 14.7486 - val_loss: 149.3212 - val_mse: 149.3212 - val_mae: 10.3725\n",
       "Epoch 3/75\n",
-      "404/404 [==============================] - 0s 344us/sample - loss: 56.6755 - mse: 56.6755 - mae: 5.4817 - val_loss: 39.1997 - val_mse: 39.1997 - val_mae: 4.9872\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 75.4833 - mse: 75.4833 - mae: 6.4003 - val_loss: 56.1032 - val_mse: 56.1032 - val_mae: 5.7075\n",
       "Epoch 4/75\n",
-      "404/404 [==============================] - 0s 364us/sample - loss: 28.3243 - mse: 28.3243 - mae: 3.7054 - val_loss: 26.9866 - val_mse: 26.9866 - val_mae: 4.0796\n",
+      "404/404 [==============================] - 0s 182us/sample - loss: 36.9937 - mse: 36.9937 - mae: 4.3364 - val_loss: 37.2357 - val_mse: 37.2357 - val_mae: 4.7944\n",
       "Epoch 5/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 20.5281 - mse: 20.5281 - mae: 3.1209 - val_loss: 24.6172 - val_mse: 24.6172 - val_mae: 3.8052\n",
+      "404/404 [==============================] - 0s 180us/sample - loss: 26.1584 - mse: 26.1584 - mae: 3.5333 - val_loss: 30.8515 - val_mse: 30.8515 - val_mae: 4.3740\n",
       "Epoch 6/75\n",
-      "404/404 [==============================] - 0s 393us/sample - loss: 17.9283 - mse: 17.9283 - mae: 2.8665 - val_loss: 23.6524 - val_mse: 23.6524 - val_mae: 3.6746\n",
+      "404/404 [==============================] - 0s 198us/sample - loss: 21.8525 - mse: 21.8525 - mae: 3.2586 - val_loss: 27.8313 - val_mse: 27.8313 - val_mae: 4.0938\n",
       "Epoch 7/75\n",
-      "404/404 [==============================] - 0s 440us/sample - loss: 16.9179 - mse: 16.9179 - mae: 2.8781 - val_loss: 23.4620 - val_mse: 23.4620 - val_mae: 3.5778\n",
+      "404/404 [==============================] - 0s 206us/sample - loss: 19.2219 - mse: 19.2219 - mae: 3.0294 - val_loss: 25.6748 - val_mse: 25.6748 - val_mae: 3.8208\n",
       "Epoch 8/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 15.1579 - mse: 15.1579 - mae: 2.6440 - val_loss: 24.1374 - val_mse: 24.1374 - val_mae: 3.5929\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 17.5437 - mse: 17.5437 - mae: 2.8859 - val_loss: 25.5435 - val_mse: 25.5435 - val_mae: 3.7422\n",
       "Epoch 9/75\n",
-      "404/404 [==============================] - 0s 367us/sample - loss: 14.1717 - mse: 14.1717 - mae: 2.5937 - val_loss: 24.4829 - val_mse: 24.4829 - val_mae: 3.5639\n",
+      "404/404 [==============================] - 0s 198us/sample - loss: 16.1240 - mse: 16.1240 - mae: 2.7601 - val_loss: 24.5621 - val_mse: 24.5621 - val_mae: 3.5964\n",
       "Epoch 10/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 13.5002 - mse: 13.5002 - mae: 2.5633 - val_loss: 25.0170 - val_mse: 25.0170 - val_mae: 3.5601\n",
+      "404/404 [==============================] - 0s 205us/sample - loss: 14.9264 - mse: 14.9264 - mae: 2.6858 - val_loss: 25.0101 - val_mse: 25.0101 - val_mae: 3.5463\n",
       "Epoch 11/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.8641 - mse: 12.8641 - mae: 2.4963 - val_loss: 25.1162 - val_mse: 25.1162 - val_mae: 3.5449\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 14.0505 - mse: 14.0505 - mae: 2.6020 - val_loss: 23.9233 - val_mse: 23.9233 - val_mae: 3.4313\n",
       "Epoch 12/75\n",
-      "404/404 [==============================] - 0s 351us/sample - loss: 12.4033 - mse: 12.4033 - mae: 2.5224 - val_loss: 25.0382 - val_mse: 25.0382 - val_mae: 3.4858\n",
+      "404/404 [==============================] - 0s 191us/sample - loss: 13.4166 - mse: 13.4166 - mae: 2.5333 - val_loss: 25.9138 - val_mse: 25.9138 - val_mae: 3.5215\n",
       "Epoch 13/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.2653 - mse: 12.2653 - mae: 2.4637 - val_loss: 26.7274 - val_mse: 26.7274 - val_mae: 3.6054\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 12.8477 - mse: 12.8477 - mae: 2.5083 - val_loss: 25.4262 - val_mse: 25.4262 - val_mae: 3.4384\n",
       "Epoch 14/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 11.8249 - mse: 11.8249 - mae: 2.4648 - val_loss: 25.2347 - val_mse: 25.2347 - val_mae: 3.4602\n",
+      "404/404 [==============================] - 0s 224us/sample - loss: 12.1410 - mse: 12.1410 - mae: 2.4447 - val_loss: 24.1520 - val_mse: 24.1520 - val_mae: 3.3220\n",
       "Epoch 15/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 11.3965 - mse: 11.3965 - mae: 2.4134 - val_loss: 25.3070 - val_mse: 25.3070 - val_mae: 3.4305\n",
+      "404/404 [==============================] - 0s 230us/sample - loss: 11.8707 - mse: 11.8707 - mae: 2.3897 - val_loss: 25.5138 - val_mse: 25.5138 - val_mae: 3.4099\n",
       "Epoch 16/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 11.0982 - mse: 11.0982 - mae: 2.3616 - val_loss: 25.0599 - val_mse: 25.0599 - val_mae: 3.3784\n",
+      "404/404 [==============================] - 0s 220us/sample - loss: 11.3098 - mse: 11.3098 - mae: 2.3681 - val_loss: 26.1788 - val_mse: 26.1788 - val_mae: 3.3366\n",
       "Epoch 17/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 11.1969 - mse: 11.1969 - mae: 2.3806 - val_loss: 25.1976 - val_mse: 25.1976 - val_mae: 3.3732\n",
+      "404/404 [==============================] - 0s 208us/sample - loss: 11.0956 - mse: 11.0956 - mae: 2.3556 - val_loss: 25.1831 - val_mse: 25.1831 - val_mae: 3.2551\n",
       "Epoch 18/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 10.9278 - mse: 10.9278 - mae: 2.3653 - val_loss: 24.2875 - val_mse: 24.2875 - val_mae: 3.3114\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 10.9463 - mse: 10.9463 - mae: 2.3137 - val_loss: 26.1118 - val_mse: 26.1118 - val_mae: 3.2895\n",
       "Epoch 19/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 10.5854 - mse: 10.5854 - mae: 2.3170 - val_loss: 26.1450 - val_mse: 26.1450 - val_mae: 3.3971\n",
+      "404/404 [==============================] - 0s 361us/sample - loss: 10.5413 - mse: 10.5413 - mae: 2.2821 - val_loss: 25.1537 - val_mse: 25.1537 - val_mae: 3.2698\n",
       "Epoch 20/75\n",
-      "404/404 [==============================] - 0s 401us/sample - loss: 10.2546 - mse: 10.2546 - mae: 2.2813 - val_loss: 26.5278 - val_mse: 26.5278 - val_mae: 3.4465\n",
+      "404/404 [==============================] - 0s 182us/sample - loss: 10.0049 - mse: 10.0049 - mae: 2.2457 - val_loss: 25.1134 - val_mse: 25.1134 - val_mae: 3.2448\n",
       "Epoch 21/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 10.1321 - mse: 10.1321 - mae: 2.2866 - val_loss: 24.0363 - val_mse: 24.0363 - val_mae: 3.2792\n",
+      "404/404 [==============================] - 0s 222us/sample - loss: 9.9285 - mse: 9.9285 - mae: 2.2264 - val_loss: 24.4498 - val_mse: 24.4498 - val_mae: 3.1821\n",
       "Epoch 22/75\n",
-      "404/404 [==============================] - 0s 421us/sample - loss: 9.9169 - mse: 9.9169 - mae: 2.2907 - val_loss: 23.7310 - val_mse: 23.7310 - val_mae: 3.2334\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 9.8573 - mse: 9.8573 - mae: 2.2485 - val_loss: 25.3196 - val_mse: 25.3196 - val_mae: 3.2374\n",
       "Epoch 23/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.6588 - mse: 9.6588 - mae: 2.2284 - val_loss: 23.6472 - val_mse: 23.6472 - val_mae: 3.2013\n",
+      "404/404 [==============================] - 0s 190us/sample - loss: 9.6289 - mse: 9.6289 - mae: 2.1992 - val_loss: 24.1484 - val_mse: 24.1484 - val_mae: 3.1423\n",
       "Epoch 24/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 9.6887 - mse: 9.6887 - mae: 2.2468 - val_loss: 23.5379 - val_mse: 23.5379 - val_mae: 3.1921\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 9.3005 - mse: 9.3005 - mae: 2.1764 - val_loss: 25.6075 - val_mse: 25.6075 - val_mae: 3.1868\n",
       "Epoch 25/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 9.4049 - mse: 9.4049 - mae: 2.1999 - val_loss: 23.7713 - val_mse: 23.7713 - val_mae: 3.2273\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 9.3483 - mse: 9.3483 - mae: 2.1722 - val_loss: 24.5594 - val_mse: 24.5594 - val_mae: 3.1655\n",
       "Epoch 26/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 9.2304 - mse: 9.2304 - mae: 2.1946 - val_loss: 23.5093 - val_mse: 23.5093 - val_mae: 3.2072\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 9.0637 - mse: 9.0637 - mae: 2.1514 - val_loss: 23.0267 - val_mse: 23.0267 - val_mae: 3.0623\n",
       "Epoch 27/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.0493 - mse: 9.0493 - mae: 2.1528 - val_loss: 23.7969 - val_mse: 23.7969 - val_mae: 3.2005\n",
+      "404/404 [==============================] - 0s 186us/sample - loss: 9.1571 - mse: 9.1571 - mae: 2.1691 - val_loss: 23.3072 - val_mse: 23.3072 - val_mae: 3.0732\n",
       "Epoch 28/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 8.9363 - mse: 8.9363 - mae: 2.1475 - val_loss: 22.1030 - val_mse: 22.1030 - val_mae: 3.0707\n",
+      "404/404 [==============================] - 0s 201us/sample - loss: 8.7309 - mse: 8.7309 - mae: 2.1040 - val_loss: 23.8722 - val_mse: 23.8722 - val_mae: 3.1149\n",
       "Epoch 29/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 8.7834 - mse: 8.7834 - mae: 2.1231 - val_loss: 22.5153 - val_mse: 22.5153 - val_mae: 3.1532\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 8.6872 - mse: 8.6872 - mae: 2.0952 - val_loss: 24.1689 - val_mse: 24.1689 - val_mae: 3.1189\n",
       "Epoch 30/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.7925 - mse: 8.7925 - mae: 2.1531 - val_loss: 22.0449 - val_mse: 22.0449 - val_mae: 3.1245\n",
+      "404/404 [==============================] - 0s 195us/sample - loss: 8.7647 - mse: 8.7647 - mae: 2.0971 - val_loss: 22.1914 - val_mse: 22.1914 - val_mae: 2.9925\n",
       "Epoch 31/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 9.1879 - mse: 9.1879 - mae: 2.2029 - val_loss: 22.1780 - val_mse: 22.1780 - val_mae: 3.0623\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 8.4746 - mse: 8.4746 - mae: 2.0879 - val_loss: 22.4664 - val_mse: 22.4664 - val_mae: 3.0504\n",
       "Epoch 32/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.7136 - mse: 8.7136 - mae: 2.1164 - val_loss: 21.9815 - val_mse: 21.9815 - val_mae: 3.0969\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 8.3266 - mse: 8.3266 - mae: 2.0441 - val_loss: 23.5687 - val_mse: 23.5687 - val_mae: 3.0316\n",
       "Epoch 33/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.3018 - mse: 8.3018 - mae: 2.0639 - val_loss: 21.0477 - val_mse: 21.0477 - val_mae: 2.9645\n",
+      "404/404 [==============================] - 0s 198us/sample - loss: 8.4078 - mse: 8.4078 - mae: 2.0931 - val_loss: 22.5758 - val_mse: 22.5758 - val_mae: 3.0012\n",
       "Epoch 34/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 8.4156 - mse: 8.4156 - mae: 2.0970 - val_loss: 22.6659 - val_mse: 22.6659 - val_mae: 3.1235\n",
+      "404/404 [==============================] - 0s 214us/sample - loss: 8.2054 - mse: 8.2054 - mae: 2.0360 - val_loss: 23.4102 - val_mse: 23.4102 - val_mae: 3.0542\n",
       "Epoch 35/75\n",
-      "404/404 [==============================] - 0s 350us/sample - loss: 8.2938 - mse: 8.2938 - mae: 2.0567 - val_loss: 20.9574 - val_mse: 20.9574 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 210us/sample - loss: 8.2101 - mse: 8.2101 - mae: 2.0498 - val_loss: 22.4434 - val_mse: 22.4434 - val_mae: 3.0206\n",
       "Epoch 36/75\n",
-      "404/404 [==============================] - 0s 357us/sample - loss: 8.0515 - mse: 8.0515 - mae: 2.0591 - val_loss: 23.2063 - val_mse: 23.2063 - val_mae: 3.1980\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 7.9859 - mse: 7.9859 - mae: 2.0028 - val_loss: 23.2023 - val_mse: 23.2023 - val_mae: 3.0369\n",
       "Epoch 37/75\n",
-      "404/404 [==============================] - 0s 381us/sample - loss: 8.1403 - mse: 8.1403 - mae: 2.0584 - val_loss: 24.5238 - val_mse: 24.5237 - val_mae: 3.3531\n",
+      "404/404 [==============================] - 0s 198us/sample - loss: 7.8717 - mse: 7.8717 - mae: 1.9760 - val_loss: 22.0084 - val_mse: 22.0084 - val_mae: 2.9336\n",
       "Epoch 38/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 8.0043 - mse: 8.0043 - mae: 2.0776 - val_loss: 22.5424 - val_mse: 22.5424 - val_mae: 3.1494\n",
+      "404/404 [==============================] - 0s 202us/sample - loss: 7.6661 - mse: 7.6661 - mae: 1.9726 - val_loss: 21.8538 - val_mse: 21.8538 - val_mae: 2.9567\n",
       "Epoch 39/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.1182 - mse: 8.1182 - mae: 2.0683 - val_loss: 19.7576 - val_mse: 19.7576 - val_mae: 2.8799\n",
+      "404/404 [==============================] - 0s 207us/sample - loss: 7.4976 - mse: 7.4976 - mae: 1.9685 - val_loss: 21.7075 - val_mse: 21.7075 - val_mae: 2.9802\n",
       "Epoch 40/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 7.8578 - mse: 7.8578 - mae: 2.0131 - val_loss: 20.7728 - val_mse: 20.7728 - val_mae: 2.9499\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 7.5221 - mse: 7.5221 - mae: 1.9404 - val_loss: 21.8232 - val_mse: 21.8232 - val_mae: 2.9254\n",
       "Epoch 41/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 7.5711 - mse: 7.5711 - mae: 1.9896 - val_loss: 20.6170 - val_mse: 20.6170 - val_mae: 2.9936\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 7.3430 - mse: 7.3430 - mae: 1.9212 - val_loss: 22.7208 - val_mse: 22.7208 - val_mae: 3.0540\n",
       "Epoch 42/75\n",
-      "404/404 [==============================] - 0s 385us/sample - loss: 7.5822 - mse: 7.5822 - mae: 1.9683 - val_loss: 20.8541 - val_mse: 20.8541 - val_mae: 3.0054\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 7.2904 - mse: 7.2904 - mae: 1.9211 - val_loss: 21.5842 - val_mse: 21.5842 - val_mae: 2.8931\n",
       "Epoch 43/75\n",
-      "404/404 [==============================] - 0s 408us/sample - loss: 7.4533 - mse: 7.4533 - mae: 1.9645 - val_loss: 20.4473 - val_mse: 20.4473 - val_mae: 2.8861\n",
+      "404/404 [==============================] - 0s 221us/sample - loss: 7.2870 - mse: 7.2870 - mae: 1.9279 - val_loss: 22.2816 - val_mse: 22.2816 - val_mae: 2.9621\n",
       "Epoch 44/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 7.5226 - mse: 7.5226 - mae: 1.9509 - val_loss: 20.5193 - val_mse: 20.5193 - val_mae: 2.9619\n",
+      "404/404 [==============================] - 0s 226us/sample - loss: 7.1994 - mse: 7.1994 - mae: 1.9203 - val_loss: 20.8573 - val_mse: 20.8573 - val_mae: 2.8988\n",
       "Epoch 45/75\n",
-      "404/404 [==============================] - 0s 355us/sample - loss: 7.2819 - mse: 7.2819 - mae: 1.9350 - val_loss: 21.4862 - val_mse: 21.4862 - val_mae: 2.9908\n",
+      "404/404 [==============================] - 0s 233us/sample - loss: 6.9650 - mse: 6.9650 - mae: 1.8790 - val_loss: 22.1440 - val_mse: 22.1440 - val_mae: 2.9251\n",
       "Epoch 46/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 7.0130 - mse: 7.0130 - mae: 1.9152 - val_loss: 20.1577 - val_mse: 20.1577 - val_mae: 2.9370\n",
+      "404/404 [==============================] - 0s 207us/sample - loss: 7.0209 - mse: 7.0209 - mae: 1.9279 - val_loss: 22.1633 - val_mse: 22.1633 - val_mae: 2.9880\n",
       "Epoch 47/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.9431 - mse: 6.9431 - mae: 1.8819 - val_loss: 21.1210 - val_mse: 21.1210 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 247us/sample - loss: 6.8293 - mse: 6.8293 - mae: 1.8509 - val_loss: 19.9882 - val_mse: 19.9882 - val_mae: 2.7929\n",
       "Epoch 48/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 6.8982 - mse: 6.8982 - mae: 1.9037 - val_loss: 19.2999 - val_mse: 19.2999 - val_mae: 2.8638\n",
+      "404/404 [==============================] - 0s 206us/sample - loss: 6.9535 - mse: 6.9535 - mae: 1.9046 - val_loss: 19.7700 - val_mse: 19.7700 - val_mae: 2.7963\n",
       "Epoch 49/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 6.9521 - mse: 6.9521 - mae: 1.8862 - val_loss: 20.7825 - val_mse: 20.7825 - val_mae: 2.9369\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 6.6473 - mse: 6.6473 - mae: 1.8567 - val_loss: 20.0766 - val_mse: 20.0766 - val_mae: 2.8179\n",
       "Epoch 50/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.8718 - mse: 6.8718 - mae: 1.8889 - val_loss: 20.0288 - val_mse: 20.0288 - val_mae: 2.8915\n",
+      "404/404 [==============================] - 0s 201us/sample - loss: 6.8562 - mse: 6.8562 - mae: 1.9118 - val_loss: 19.1089 - val_mse: 19.1089 - val_mae: 2.7972\n",
       "Epoch 51/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 6.7111 - mse: 6.7111 - mae: 1.8702 - val_loss: 20.4913 - val_mse: 20.4913 - val_mae: 3.0116\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 6.6105 - mse: 6.6105 - mae: 1.8149 - val_loss: 21.4676 - val_mse: 21.4676 - val_mae: 2.8901\n",
       "Epoch 52/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 6.7492 - mse: 6.7492 - mae: 1.8482 - val_loss: 18.3008 - val_mse: 18.3008 - val_mae: 2.7362\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 6.6635 - mse: 6.6635 - mae: 1.8672 - val_loss: 19.0941 - val_mse: 19.0942 - val_mae: 2.7605\n",
       "Epoch 53/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.6262 - mse: 6.6262 - mae: 1.8395 - val_loss: 18.1885 - val_mse: 18.1885 - val_mae: 2.6920\n",
+      "404/404 [==============================] - 0s 218us/sample - loss: 6.5160 - mse: 6.5160 - mae: 1.8192 - val_loss: 20.0609 - val_mse: 20.0609 - val_mae: 2.8714\n",
       "Epoch 54/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 6.7148 - mse: 6.7148 - mae: 1.8611 - val_loss: 18.5764 - val_mse: 18.5764 - val_mae: 2.6977\n",
+      "404/404 [==============================] - 0s 204us/sample - loss: 6.3640 - mse: 6.3640 - mae: 1.8043 - val_loss: 18.1585 - val_mse: 18.1585 - val_mae: 2.7094\n",
       "Epoch 55/75\n",
-      "404/404 [==============================] - 0s 358us/sample - loss: 6.5425 - mse: 6.5425 - mae: 1.8522 - val_loss: 19.5772 - val_mse: 19.5772 - val_mae: 2.8326\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 6.4613 - mse: 6.4613 - mae: 1.8400 - val_loss: 18.8502 - val_mse: 18.8502 - val_mae: 2.7606\n",
       "Epoch 56/75\n",
-      "404/404 [==============================] - 0s 423us/sample - loss: 6.3349 - mse: 6.3349 - mae: 1.8175 - val_loss: 19.0932 - val_mse: 19.0932 - val_mae: 2.8260\n",
+      "404/404 [==============================] - 0s 199us/sample - loss: 6.1402 - mse: 6.1402 - mae: 1.7718 - val_loss: 19.1818 - val_mse: 19.1818 - val_mae: 2.8306\n",
       "Epoch 57/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.4253 - mse: 6.4253 - mae: 1.7972 - val_loss: 20.4036 - val_mse: 20.4036 - val_mae: 2.9258\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 6.3169 - mse: 6.3169 - mae: 1.7999 - val_loss: 19.5459 - val_mse: 19.5459 - val_mae: 2.8296\n",
       "Epoch 58/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.2897 - mse: 6.2897 - mae: 1.7785 - val_loss: 21.2845 - val_mse: 21.2845 - val_mae: 3.0715\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 5.9515 - mse: 5.9515 - mae: 1.7416 - val_loss: 18.7167 - val_mse: 18.7167 - val_mae: 2.7009\n",
       "Epoch 59/75\n",
-      "404/404 [==============================] - 0s 378us/sample - loss: 6.7839 - mse: 6.7839 - mae: 1.9027 - val_loss: 18.6853 - val_mse: 18.6853 - val_mae: 2.7709\n",
+      "404/404 [==============================] - 0s 185us/sample - loss: 6.4652 - mse: 6.4652 - mae: 1.8382 - val_loss: 17.9912 - val_mse: 17.9912 - val_mae: 2.7436\n",
       "Epoch 60/75\n",
-      "404/404 [==============================] - 0s 395us/sample - loss: 6.7178 - mse: 6.7178 - mae: 1.8871 - val_loss: 19.5394 - val_mse: 19.5394 - val_mae: 2.8101\n",
+      "404/404 [==============================] - 0s 181us/sample - loss: 5.8052 - mse: 5.8052 - mae: 1.7236 - val_loss: 20.5604 - val_mse: 20.5604 - val_mae: 2.9333\n",
       "Epoch 61/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 6.4152 - mse: 6.4152 - mae: 1.8175 - val_loss: 18.2377 - val_mse: 18.2377 - val_mae: 2.7450\n",
+      "404/404 [==============================] - 0s 213us/sample - loss: 5.8701 - mse: 5.8701 - mae: 1.7353 - val_loss: 18.9493 - val_mse: 18.9493 - val_mae: 2.7612\n",
       "Epoch 62/75\n",
-      "404/404 [==============================] - 0s 384us/sample - loss: 5.9727 - mse: 5.9727 - mae: 1.7630 - val_loss: 19.0252 - val_mse: 19.0252 - val_mae: 2.7960\n",
+      "404/404 [==============================] - 0s 218us/sample - loss: 5.7786 - mse: 5.7786 - mae: 1.7308 - val_loss: 17.9815 - val_mse: 17.9815 - val_mae: 2.6992\n",
       "Epoch 63/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 6.0973 - mse: 6.0973 - mae: 1.8071 - val_loss: 18.8069 - val_mse: 18.8069 - val_mae: 2.8894\n",
+      "404/404 [==============================] - 0s 367us/sample - loss: 5.7606 - mse: 5.7606 - mae: 1.7215 - val_loss: 18.2646 - val_mse: 18.2646 - val_mae: 2.7523\n",
       "Epoch 64/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.1074 - mse: 6.1074 - mae: 1.7978 - val_loss: 18.4702 - val_mse: 18.4702 - val_mae: 2.7851\n",
+      "404/404 [==============================] - 0s 201us/sample - loss: 5.5298 - mse: 5.5298 - mae: 1.7207 - val_loss: 17.3665 - val_mse: 17.3665 - val_mae: 2.6704\n",
       "Epoch 65/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 5.9329 - mse: 5.9329 - mae: 1.7545 - val_loss: 18.5321 - val_mse: 18.5321 - val_mae: 2.7933\n",
+      "404/404 [==============================] - 0s 207us/sample - loss: 5.7095 - mse: 5.7095 - mae: 1.7240 - val_loss: 17.7988 - val_mse: 17.7988 - val_mae: 2.7112\n",
       "Epoch 66/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.7473 - mse: 5.7473 - mae: 1.7211 - val_loss: 18.5536 - val_mse: 18.5536 - val_mae: 2.8010\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 5.4605 - mse: 5.4605 - mae: 1.6752 - val_loss: 17.8101 - val_mse: 17.8101 - val_mae: 2.7046\n",
       "Epoch 67/75\n",
-      "404/404 [==============================] - 0s 339us/sample - loss: 5.8866 - mse: 5.8866 - mae: 1.7224 - val_loss: 18.0067 - val_mse: 18.0067 - val_mae: 2.7054\n",
+      "404/404 [==============================] - 0s 190us/sample - loss: 5.6886 - mse: 5.6886 - mae: 1.7360 - val_loss: 18.4368 - val_mse: 18.4368 - val_mae: 2.7304\n",
       "Epoch 68/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.7885 - mse: 5.7885 - mae: 1.7391 - val_loss: 17.5502 - val_mse: 17.5502 - val_mae: 2.6767\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 5.6463 - mse: 5.6463 - mae: 1.7258 - val_loss: 17.9863 - val_mse: 17.9863 - val_mae: 2.7348\n",
       "Epoch 69/75\n",
-      "404/404 [==============================] - 0s 331us/sample - loss: 5.8809 - mse: 5.8809 - mae: 1.7542 - val_loss: 17.0280 - val_mse: 17.0280 - val_mae: 2.6404\n",
+      "404/404 [==============================] - 0s 190us/sample - loss: 5.4167 - mse: 5.4167 - mae: 1.6569 - val_loss: 17.4666 - val_mse: 17.4666 - val_mae: 2.7435\n",
       "Epoch 70/75\n",
-      "404/404 [==============================] - 0s 343us/sample - loss: 5.6028 - mse: 5.6028 - mae: 1.6972 - val_loss: 17.7188 - val_mse: 17.7188 - val_mae: 2.6979\n",
+      "404/404 [==============================] - 0s 217us/sample - loss: 5.3313 - mse: 5.3313 - mae: 1.6677 - val_loss: 17.3793 - val_mse: 17.3793 - val_mae: 2.7352\n",
       "Epoch 71/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.4361 - mse: 5.4361 - mae: 1.6741 - val_loss: 16.8852 - val_mse: 16.8852 - val_mae: 2.6126\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 5.3521 - mse: 5.3521 - mae: 1.6727 - val_loss: 17.5926 - val_mse: 17.5926 - val_mae: 2.7449\n",
       "Epoch 72/75\n",
-      "404/404 [==============================] - 0s 345us/sample - loss: 5.5608 - mse: 5.5608 - mae: 1.7252 - val_loss: 16.7483 - val_mse: 16.7483 - val_mae: 2.6063\n",
+      "404/404 [==============================] - 0s 206us/sample - loss: 5.1023 - mse: 5.1023 - mae: 1.6179 - val_loss: 17.0872 - val_mse: 17.0872 - val_mae: 2.6702\n",
       "Epoch 73/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.5022 - mse: 5.5022 - mae: 1.6912 - val_loss: 17.6786 - val_mse: 17.6786 - val_mae: 2.7316\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 5.1536 - mse: 5.1536 - mae: 1.6135 - val_loss: 16.7142 - val_mse: 16.7142 - val_mae: 2.6292\n",
       "Epoch 74/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 5.2794 - mse: 5.2794 - mae: 1.6478 - val_loss: 17.6115 - val_mse: 17.6115 - val_mae: 2.6773\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 5.2114 - mse: 5.2114 - mae: 1.6812 - val_loss: 17.9796 - val_mse: 17.9796 - val_mae: 2.7837\n",
       "Epoch 75/75\n",
-      "404/404 [==============================] - 0s 338us/sample - loss: 5.4796 - mse: 5.4796 - mae: 1.6876 - val_loss: 17.2835 - val_mse: 17.2835 - val_mae: 2.7126\n"
+      "404/404 [==============================] - 0s 192us/sample - loss: 4.9884 - mse: 4.9884 - mae: 1.6254 - val_loss: 16.7177 - val_mse: 16.7177 - val_mae: 2.7114\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f36340c6b38>"
+       "<tensorflow.python.keras.callbacks.History at 0x13b4faeb8>"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -417,7 +413,6 @@
     "## What Hyperparameters are there to test?\n",
     "\n",
     "- batch_size\n",
-    "- training epochs\n",
     "- optimization algorithms\n",
     "- learning rate\n",
     "- momentum\n",
@@ -449,7 +444,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 9,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -460,25 +455,17 @@
     "outputId": "ae996575-78e2-43fb-9dbe-5d44aaf0b430"
    },
    "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
-      "  warnings.warn(CV_WARNING, FutureWarning)\n"
-     ]
-    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Best: 0.65234375 using {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.65234375, Stdev: 0.033298728782667764 with: {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6263020833333334, Stdev: 0.01813592223591682 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6041666666666666, Stdev: 0.037782859709757574 with: {'batch_size': 40, 'epochs': 20}\n",
-      "Means: 0.5533854166666666, Stdev: 0.03210632293213009 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.61328125, Stdev: 0.024079742199097563 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.5611979166666666, Stdev: 0.038450060052691144 with: {'batch_size': 100, 'epochs': 20}\n"
+      "Best: 0.6550632357597351 using {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6326627731323242, Stdev: 0.053739447760642364 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6550632357597351, Stdev: 0.03873597023264761 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6042356491088867, Stdev: 0.033962453537288856 with: {'batch_size': 40, 'epochs': 20}\n",
+      "Means: 0.6262965798377991, Stdev: 0.02364799838304432 with: {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.5924199938774108, Stdev: 0.023305833006304417 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.5298192083835602, Stdev: 0.06749446690447318 with: {'batch_size': 100, 'epochs': 20}\n"
      ]
     }
    ],
@@ -537,61 +524,6 @@
     "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {
-    "colab_type": "text",
-    "id": "pmABfjlvXbqi"
-   },
-   "source": [
-    "## Epochs\n",
-    "\n",
-    "The number of training epochs has a large and direct affect on the accuracy, However, more epochs is almost always goign to better than less epochs. This means that if you tune this parameter at the beginning and try and maintain the same value all throughout your training, you're going to be waiting a long time for each iteration of GridSearch. I suggest picking a fixed moderat # of epochs all throughout your training and then Grid Searching this parameter at the very end. "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {
-    "colab": {
-     "base_uri": "https://localhost:8080/",
-     "height": 26329
-    },
-    "colab_type": "code",
-    "id": "bAmxP3N7TmFh",
-    "outputId": "3ddb08c4-51ac-4eaa-ff39-143397024544"
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Best: 0.7044270833333334 using {'batch_size': 20, 'epochs': 200}\n",
-      "Means: 0.6666666666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6588541666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 40}\n",
-      "Means: 0.6848958333333334, Stdev: 0.03498705427745938 with: {'batch_size': 20, 'epochs': 60}\n",
-      "Means: 0.7044270833333334, Stdev: 0.018414239093399672 with: {'batch_size': 20, 'epochs': 200}\n"
-     ]
-    }
-   ],
-   "source": [
-    "# define the grid search parameters\n",
-    "param_grid = {'batch_size': [20],\n",
-    "              'epochs': [20, 40, 60,200]}\n",
-    "\n",
-    "# Create Grid Search\n",
-    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
-    "grid_result = grid.fit(X, Y)\n",
-    "\n",
-    "# Report Results\n",
-    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
-    "means = grid_result.cv_results_['mean_test_score']\n",
-    "stds = grid_result.cv_results_['std_test_score']\n",
-    "params = grid_result.cv_results_['params']\n",
-    "for mean, stdev, param in zip(means, stds, params):\n",
-    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -604,6 +536,13 @@
     "Remember that there's a different optimizers [optimizers](https://keras.io/optimizers/). At some point, take some time to read up on them a little bit. \"adam\" usually gives the best results. The thing to know about choosing an optimizer is that different optimizers have different hyperparameters like learning rate, momentum, etc. So based on the optimizer you choose you might also have to tune the learning rate and momentum of those optimizers after that. "
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -730,44 +669,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 5,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "W&B Run: https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro"
-      ]
-     },
-     "execution_count": 6,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "import wandb\n",
-    "from wandb.keras import WandbCallback"
+    "from wandb.keras import WandbCallback\n",
+    "from tensorflow.keras.optimizers import Adam"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 6,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -783,8 +696,8 @@
       "text/html": [
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31</a><br/>\n",
+       "                Project page: <a href=\"https://app.wandb.ai/ds8/ds11_inclass\" target=\"_blank\">https://app.wandb.ai/ds8/ds11_inclass</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/ds8/ds11_inclass/runs/14b3b0u3\" target=\"_blank\">https://app.wandb.ai/ds8/ds11_inclass/runs/14b3b0u3</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -800,120 +713,121 @@
      "text": [
       "Train on 270 samples, validate on 134 samples\n",
       "Epoch 1/50\n",
-      "270/270 [==============================] - 1s 3ms/sample - loss: 492.3539 - mse: 492.3539 - mae: 20.3197 - val_loss: 481.5445 - val_mse: 481.5445 - val_mae: 19.6138\n",
+      "270/270 [==============================] - 0s 2ms/sample - loss: 138.9145 - mse: 138.9145 - mae: 8.9500 - val_loss: 51.7144 - val_mse: 51.7144 - val_mae: 5.2122\n",
       "Epoch 2/50\n",
-      "270/270 [==============================] - 0s 591us/sample - loss: 239.4999 - mse: 239.4999 - mae: 12.8064 - val_loss: 113.8561 - val_mse: 113.8561 - val_mae: 8.2962\n",
+      "270/270 [==============================] - 0s 324us/sample - loss: 21.8397 - mse: 21.8397 - mae: 3.4192 - val_loss: 29.2580 - val_mse: 29.2580 - val_mae: 3.7363\n",
       "Epoch 3/50\n",
-      "270/270 [==============================] - 0s 618us/sample - loss: 56.2921 - mse: 56.2921 - mae: 5.8988 - val_loss: 62.7912 - val_mse: 62.7912 - val_mae: 5.6465\n",
+      "270/270 [==============================] - 0s 292us/sample - loss: 15.0002 - mse: 15.0002 - mae: 2.9240 - val_loss: 25.3561 - val_mse: 25.3561 - val_mae: 3.8415\n",
       "Epoch 4/50\n",
-      "270/270 [==============================] - 0s 613us/sample - loss: 29.4994 - mse: 29.4994 - mae: 3.9653 - val_loss: 37.9256 - val_mse: 37.9256 - val_mae: 4.1730\n",
+      "270/270 [==============================] - 0s 296us/sample - loss: 11.6298 - mse: 11.6298 - mae: 2.6585 - val_loss: 19.6715 - val_mse: 19.6715 - val_mae: 3.0356\n",
       "Epoch 5/50\n",
-      "270/270 [==============================] - 0s 608us/sample - loss: 20.6919 - mse: 20.6919 - mae: 3.3022 - val_loss: 31.7489 - val_mse: 31.7489 - val_mae: 3.7113\n",
+      "270/270 [==============================] - 0s 225us/sample - loss: 14.2993 - mse: 14.2993 - mae: 2.8793 - val_loss: 32.6452 - val_mse: 32.6452 - val_mae: 4.1274\n",
       "Epoch 6/50\n",
-      "270/270 [==============================] - 0s 602us/sample - loss: 17.2701 - mse: 17.2701 - mae: 3.0291 - val_loss: 27.3921 - val_mse: 27.3921 - val_mae: 3.4958\n",
+      "270/270 [==============================] - 0s 241us/sample - loss: 13.4703 - mse: 13.4703 - mae: 2.8416 - val_loss: 21.2425 - val_mse: 21.2425 - val_mae: 3.3646\n",
       "Epoch 7/50\n",
-      "270/270 [==============================] - 0s 671us/sample - loss: 15.5172 - mse: 15.5172 - mae: 2.8537 - val_loss: 25.3208 - val_mse: 25.3208 - val_mae: 3.3650\n",
+      "270/270 [==============================] - 0s 282us/sample - loss: 11.6889 - mse: 11.6889 - mae: 2.5908 - val_loss: 19.2724 - val_mse: 19.2724 - val_mae: 3.1393\n",
       "Epoch 8/50\n",
-      "270/270 [==============================] - 0s 661us/sample - loss: 13.7548 - mse: 13.7548 - mae: 2.7089 - val_loss: 23.8920 - val_mse: 23.8920 - val_mae: 3.2746\n",
+      "270/270 [==============================] - 0s 296us/sample - loss: 13.5004 - mse: 13.5004 - mae: 2.8492 - val_loss: 18.3513 - val_mse: 18.3513 - val_mae: 2.9234\n",
       "Epoch 9/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 12.3745 - mse: 12.3745 - mae: 2.5662 - val_loss: 22.1294 - val_mse: 22.1294 - val_mae: 3.1509\n",
+      "270/270 [==============================] - 0s 202us/sample - loss: 11.2053 - mse: 11.2053 - mae: 2.5419 - val_loss: 33.1416 - val_mse: 33.1416 - val_mae: 3.9312\n",
       "Epoch 10/50\n",
-      "270/270 [==============================] - 0s 614us/sample - loss: 11.2424 - mse: 11.2424 - mae: 2.4804 - val_loss: 20.5718 - val_mse: 20.5718 - val_mae: 3.0461\n",
+      "270/270 [==============================] - 0s 291us/sample - loss: 13.7186 - mse: 13.7186 - mae: 2.8729 - val_loss: 17.0216 - val_mse: 17.0216 - val_mae: 2.7361\n",
       "Epoch 11/50\n",
-      "270/270 [==============================] - 0s 605us/sample - loss: 10.6098 - mse: 10.6098 - mae: 2.4178 - val_loss: 20.3467 - val_mse: 20.3467 - val_mae: 3.0251\n",
+      "270/270 [==============================] - 0s 219us/sample - loss: 10.3775 - mse: 10.3775 - mae: 2.4925 - val_loss: 23.6602 - val_mse: 23.6602 - val_mae: 3.6209\n",
       "Epoch 12/50\n",
-      "270/270 [==============================] - 0s 576us/sample - loss: 10.0011 - mse: 10.0011 - mae: 2.3257 - val_loss: 18.4283 - val_mse: 18.4283 - val_mae: 2.8938\n",
+      "270/270 [==============================] - 0s 308us/sample - loss: 11.4038 - mse: 11.4038 - mae: 2.5401 - val_loss: 14.8514 - val_mse: 14.8514 - val_mae: 2.5994\n",
       "Epoch 13/50\n",
-      "270/270 [==============================] - 0s 666us/sample - loss: 9.1287 - mse: 9.1287 - mae: 2.2384 - val_loss: 18.2024 - val_mse: 18.2024 - val_mae: 2.9116\n",
+      "270/270 [==============================] - 0s 233us/sample - loss: 7.4620 - mse: 7.4620 - mae: 2.0954 - val_loss: 20.7310 - val_mse: 20.7310 - val_mae: 3.1418\n",
       "Epoch 14/50\n",
-      "270/270 [==============================] - 0s 603us/sample - loss: 8.6211 - mse: 8.6211 - mae: 2.1980 - val_loss: 17.4749 - val_mse: 17.4749 - val_mae: 2.8290\n",
+      "270/270 [==============================] - 0s 286us/sample - loss: 7.1697 - mse: 7.1697 - mae: 2.0695 - val_loss: 14.5003 - val_mse: 14.5003 - val_mae: 2.5324\n",
       "Epoch 15/50\n",
-      "270/270 [==============================] - 0s 463us/sample - loss: 8.4558 - mse: 8.4558 - mae: 2.2087 - val_loss: 17.7878 - val_mse: 17.7878 - val_mae: 2.8516\n",
+      "270/270 [==============================] - 0s 235us/sample - loss: 5.8991 - mse: 5.8991 - mae: 1.7817 - val_loss: 14.8380 - val_mse: 14.8380 - val_mae: 2.5976\n",
       "Epoch 16/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 8.3626 - mse: 8.3626 - mae: 2.2031 - val_loss: 16.7101 - val_mse: 16.7101 - val_mae: 2.7820\n",
+      "270/270 [==============================] - 0s 221us/sample - loss: 6.5491 - mse: 6.5491 - mae: 1.9174 - val_loss: 17.5180 - val_mse: 17.5180 - val_mae: 2.8652\n",
       "Epoch 17/50\n",
-      "270/270 [==============================] - 0s 607us/sample - loss: 7.9180 - mse: 7.9180 - mae: 2.1265 - val_loss: 16.6064 - val_mse: 16.6064 - val_mae: 2.7419\n",
+      "270/270 [==============================] - 0s 226us/sample - loss: 7.1756 - mse: 7.1756 - mae: 2.0015 - val_loss: 17.6014 - val_mse: 17.6014 - val_mae: 2.9419\n",
       "Epoch 18/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 7.5552 - mse: 7.5552 - mae: 2.0235 - val_loss: 17.2872 - val_mse: 17.2872 - val_mae: 2.8539\n",
+      "270/270 [==============================] - 0s 214us/sample - loss: 9.4907 - mse: 9.4907 - mae: 2.2941 - val_loss: 15.6020 - val_mse: 15.6020 - val_mae: 2.5410\n",
       "Epoch 19/50\n",
-      "270/270 [==============================] - 0s 616us/sample - loss: 7.0971 - mse: 7.0971 - mae: 2.0038 - val_loss: 16.5110 - val_mse: 16.5110 - val_mae: 2.8042\n",
+      "270/270 [==============================] - 0s 295us/sample - loss: 6.2347 - mse: 6.2347 - mae: 1.8593 - val_loss: 13.9455 - val_mse: 13.9455 - val_mae: 2.4743\n",
       "Epoch 20/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 6.7068 - mse: 6.7068 - mae: 1.9539 - val_loss: 15.5886 - val_mse: 15.5886 - val_mae: 2.7048\n",
+      "270/270 [==============================] - 0s 220us/sample - loss: 7.4222 - mse: 7.4222 - mae: 2.0721 - val_loss: 19.2021 - val_mse: 19.2022 - val_mae: 2.8867\n",
       "Epoch 21/50\n",
-      "270/270 [==============================] - 0s 461us/sample - loss: 6.8542 - mse: 6.8542 - mae: 1.9979 - val_loss: 17.2378 - val_mse: 17.2378 - val_mae: 2.8853\n",
+      "270/270 [==============================] - 0s 239us/sample - loss: 7.3779 - mse: 7.3779 - mae: 2.0137 - val_loss: 14.5067 - val_mse: 14.5067 - val_mae: 2.6077\n",
       "Epoch 22/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 6.5719 - mse: 6.5719 - mae: 1.9312 - val_loss: 16.3043 - val_mse: 16.3043 - val_mae: 2.7756\n",
+      "270/270 [==============================] - 0s 230us/sample - loss: 11.6563 - mse: 11.6563 - mae: 2.6735 - val_loss: 39.5495 - val_mse: 39.5495 - val_mae: 5.0733\n",
       "Epoch 23/50\n",
-      "270/270 [==============================] - 0s 478us/sample - loss: 6.6161 - mse: 6.6161 - mae: 1.9572 - val_loss: 15.7992 - val_mse: 15.7992 - val_mae: 2.7219\n",
+      "270/270 [==============================] - 0s 230us/sample - loss: 13.7820 - mse: 13.7820 - mae: 2.9427 - val_loss: 26.3273 - val_mse: 26.3273 - val_mae: 3.9662\n",
       "Epoch 24/50\n",
-      "270/270 [==============================] - 0s 491us/sample - loss: 7.1269 - mse: 7.1269 - mae: 2.0137 - val_loss: 16.5402 - val_mse: 16.5402 - val_mae: 2.8005\n",
+      "270/270 [==============================] - 0s 222us/sample - loss: 14.0290 - mse: 14.0290 - mae: 2.8914 - val_loss: 25.7985 - val_mse: 25.7985 - val_mae: 3.7610\n",
       "Epoch 25/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 6.3382 - mse: 6.3382 - mae: 1.8540 - val_loss: 16.5034 - val_mse: 16.5034 - val_mae: 2.7864\n",
+      "270/270 [==============================] - 0s 214us/sample - loss: 24.0668 - mse: 24.0668 - mae: 3.7455 - val_loss: 25.1263 - val_mse: 25.1263 - val_mae: 4.0084\n",
       "Epoch 26/50\n",
-      "270/270 [==============================] - 0s 488us/sample - loss: 5.9442 - mse: 5.9442 - mae: 1.8251 - val_loss: 15.6558 - val_mse: 15.6558 - val_mae: 2.7102\n",
+      "270/270 [==============================] - 0s 281us/sample - loss: 9.9335 - mse: 9.9335 - mae: 2.3678 - val_loss: 16.0276 - val_mse: 16.0276 - val_mae: 2.6383\n",
       "Epoch 27/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 5.5832 - mse: 5.5832 - mae: 1.7432 - val_loss: 15.3021 - val_mse: 15.3021 - val_mae: 2.6862\n",
+      "270/270 [==============================] - 0s 226us/sample - loss: 5.4482 - mse: 5.4482 - mae: 1.7886 - val_loss: 14.1211 - val_mse: 14.1211 - val_mae: 2.5085\n",
       "Epoch 28/50\n",
-      "270/270 [==============================] - 0s 436us/sample - loss: 5.4530 - mse: 5.4530 - mae: 1.7354 - val_loss: 15.4570 - val_mse: 15.4570 - val_mae: 2.6846\n",
+      "270/270 [==============================] - 0s 314us/sample - loss: 5.3337 - mse: 5.3337 - mae: 1.7782 - val_loss: 19.4290 - val_mse: 19.4290 - val_mae: 3.3452\n",
       "Epoch 29/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 5.3070 - mse: 5.3070 - mae: 1.7079 - val_loss: 15.8510 - val_mse: 15.8510 - val_mae: 2.7644\n",
+      "270/270 [==============================] - 0s 297us/sample - loss: 6.1123 - mse: 6.1123 - mae: 1.9234 - val_loss: 16.7146 - val_mse: 16.7146 - val_mae: 2.8827\n",
       "Epoch 30/50\n",
-      "270/270 [==============================] - 0s 477us/sample - loss: 5.4157 - mse: 5.4157 - mae: 1.7321 - val_loss: 15.9160 - val_mse: 15.9160 - val_mae: 2.7134\n",
+      "270/270 [==============================] - 0s 280us/sample - loss: 7.5216 - mse: 7.5216 - mae: 2.1819 - val_loss: 18.2393 - val_mse: 18.2393 - val_mae: 3.3214\n",
       "Epoch 31/50\n",
-      "270/270 [==============================] - 0s 452us/sample - loss: 5.2639 - mse: 5.2639 - mae: 1.6981 - val_loss: 15.3554 - val_mse: 15.3554 - val_mae: 2.6662\n",
+      "270/270 [==============================] - 0s 276us/sample - loss: 5.9929 - mse: 5.9929 - mae: 1.8928 - val_loss: 15.5997 - val_mse: 15.5997 - val_mae: 2.7014\n",
       "Epoch 32/50\n",
-      "270/270 [==============================] - 0s 475us/sample - loss: 5.7687 - mse: 5.7687 - mae: 1.8045 - val_loss: 15.7151 - val_mse: 15.7151 - val_mae: 2.6867\n",
+      "270/270 [==============================] - 0s 247us/sample - loss: 5.7690 - mse: 5.7690 - mae: 1.7884 - val_loss: 26.1525 - val_mse: 26.1525 - val_mae: 3.7839\n",
       "Epoch 33/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 5.5210 - mse: 5.5210 - mae: 1.7367 - val_loss: 15.4227 - val_mse: 15.4227 - val_mae: 2.6561\n",
+      "270/270 [==============================] - 0s 216us/sample - loss: 10.0191 - mse: 10.0191 - mae: 2.4126 - val_loss: 14.5345 - val_mse: 14.5345 - val_mae: 2.6590\n",
       "Epoch 34/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 5.5663 - mse: 5.5663 - mae: 1.7294 - val_loss: 15.3376 - val_mse: 15.3376 - val_mae: 2.6991\n",
+      "270/270 [==============================] - 0s 299us/sample - loss: 5.8112 - mse: 5.8112 - mae: 1.8403 - val_loss: 11.9786 - val_mse: 11.9786 - val_mae: 2.2824\n",
       "Epoch 35/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 5.0063 - mse: 5.0063 - mae: 1.6196 - val_loss: 15.2642 - val_mse: 15.2642 - val_mae: 2.6796\n",
+      "270/270 [==============================] - 0s 266us/sample - loss: 5.5229 - mse: 5.5229 - mae: 1.7643 - val_loss: 14.0598 - val_mse: 14.0598 - val_mae: 2.6833\n",
       "Epoch 36/50\n",
-      "270/270 [==============================] - 0s 459us/sample - loss: 4.7251 - mse: 4.7251 - mae: 1.5727 - val_loss: 15.4858 - val_mse: 15.4858 - val_mae: 2.7288\n",
+      "270/270 [==============================] - 0s 223us/sample - loss: 5.5310 - mse: 5.5310 - mae: 1.8918 - val_loss: 14.8344 - val_mse: 14.8344 - val_mae: 2.5647\n",
       "Epoch 37/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 4.6394 - mse: 4.6394 - mae: 1.5854 - val_loss: 15.1139 - val_mse: 15.1139 - val_mae: 2.6305\n",
+      "270/270 [==============================] - 0s 236us/sample - loss: 4.8576 - mse: 4.8576 - mae: 1.6790 - val_loss: 15.6866 - val_mse: 15.6866 - val_mae: 2.4735\n",
       "Epoch 38/50\n",
-      "270/270 [==============================] - 0s 592us/sample - loss: 4.5669 - mse: 4.5669 - mae: 1.5548 - val_loss: 14.9898 - val_mse: 14.9898 - val_mae: 2.6340\n",
+      "270/270 [==============================] - 0s 267us/sample - loss: 4.7174 - mse: 4.7174 - mae: 1.6514 - val_loss: 13.0408 - val_mse: 13.0408 - val_mae: 2.5533\n",
       "Epoch 39/50\n",
-      "270/270 [==============================] - 0s 458us/sample - loss: 4.4480 - mse: 4.4480 - mae: 1.5334 - val_loss: 15.6389 - val_mse: 15.6389 - val_mae: 2.7337\n",
+      "270/270 [==============================] - 0s 235us/sample - loss: 3.9545 - mse: 3.9545 - mae: 1.4814 - val_loss: 12.0916 - val_mse: 12.0916 - val_mae: 2.3753\n",
       "Epoch 40/50\n",
-      "270/270 [==============================] - 0s 455us/sample - loss: 4.4119 - mse: 4.4119 - mae: 1.5426 - val_loss: 15.0723 - val_mse: 15.0723 - val_mae: 2.6709\n",
+      "270/270 [==============================] - 0s 221us/sample - loss: 3.8315 - mse: 3.8315 - mae: 1.4848 - val_loss: 16.1540 - val_mse: 16.1540 - val_mae: 2.9464\n",
       "Epoch 41/50\n",
-      "270/270 [==============================] - 0s 473us/sample - loss: 4.0797 - mse: 4.0797 - mae: 1.4725 - val_loss: 15.4706 - val_mse: 15.4706 - val_mae: 2.6707\n",
+      "270/270 [==============================] - 0s 253us/sample - loss: 4.4892 - mse: 4.4892 - mae: 1.5632 - val_loss: 13.1090 - val_mse: 13.1090 - val_mae: 2.4817\n",
       "Epoch 42/50\n",
-      "270/270 [==============================] - 0s 449us/sample - loss: 4.0619 - mse: 4.0619 - mae: 1.4692 - val_loss: 15.2423 - val_mse: 15.2423 - val_mae: 2.6165\n",
+      "270/270 [==============================] - 0s 206us/sample - loss: 5.5991 - mse: 5.5991 - mae: 1.7906 - val_loss: 13.7377 - val_mse: 13.7377 - val_mae: 2.5206\n",
       "Epoch 43/50\n",
-      "270/270 [==============================] - 0s 465us/sample - loss: 4.1861 - mse: 4.1861 - mae: 1.5076 - val_loss: 15.7510 - val_mse: 15.7510 - val_mae: 2.7279\n",
+      "270/270 [==============================] - 0s 234us/sample - loss: 4.4211 - mse: 4.4211 - mae: 1.5776 - val_loss: 17.0416 - val_mse: 17.0416 - val_mae: 2.6130\n",
       "Epoch 44/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 4.1128 - mse: 4.1128 - mae: 1.4810 - val_loss: 15.4814 - val_mse: 15.4814 - val_mae: 2.6562\n",
+      "270/270 [==============================] - 0s 224us/sample - loss: 7.7029 - mse: 7.7029 - mae: 2.1259 - val_loss: 17.8710 - val_mse: 17.8710 - val_mae: 3.3293\n",
       "Epoch 45/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 4.2171 - mse: 4.2171 - mae: 1.5205 - val_loss: 16.3839 - val_mse: 16.3839 - val_mae: 2.8194\n",
+      "270/270 [==============================] - 0s 261us/sample - loss: 6.0421 - mse: 6.0421 - mae: 1.9403 - val_loss: 12.0220 - val_mse: 12.0220 - val_mae: 2.5121\n",
       "Epoch 46/50\n",
-      "270/270 [==============================] - 0s 422us/sample - loss: 4.2609 - mse: 4.2609 - mae: 1.5548 - val_loss: 15.3587 - val_mse: 15.3587 - val_mae: 2.7161\n",
+      "270/270 [==============================] - 0s 247us/sample - loss: 5.6530 - mse: 5.6530 - mae: 1.7613 - val_loss: 14.8337 - val_mse: 14.8337 - val_mae: 2.4639\n",
       "Epoch 47/50\n",
-      "270/270 [==============================] - 0s 454us/sample - loss: 4.4635 - mse: 4.4635 - mae: 1.5440 - val_loss: 15.7736 - val_mse: 15.7736 - val_mae: 2.7184\n",
+      "270/270 [==============================] - 0s 225us/sample - loss: 5.5563 - mse: 5.5563 - mae: 1.8507 - val_loss: 14.9009 - val_mse: 14.9009 - val_mae: 2.3932\n",
       "Epoch 48/50\n",
-      "270/270 [==============================] - 0s 426us/sample - loss: 3.7406 - mse: 3.7406 - mae: 1.4147 - val_loss: 15.6718 - val_mse: 15.6718 - val_mae: 2.7468\n",
+      "270/270 [==============================] - 0s 226us/sample - loss: 6.4173 - mse: 6.4173 - mae: 1.8718 - val_loss: 33.3655 - val_mse: 33.3655 - val_mae: 4.3948\n",
       "Epoch 49/50\n",
-      "270/270 [==============================] - 0s 445us/sample - loss: 3.6173 - mse: 3.6173 - mae: 1.3816 - val_loss: 15.7291 - val_mse: 15.7291 - val_mae: 2.7789\n",
+      "270/270 [==============================] - 0s 208us/sample - loss: 10.6645 - mse: 10.6645 - mae: 2.5487 - val_loss: 13.1630 - val_mse: 13.1630 - val_mae: 2.4210\n",
       "Epoch 50/50\n",
-      "270/270 [==============================] - 0s 430us/sample - loss: 3.6303 - mse: 3.6303 - mae: 1.4266 - val_loss: 15.4937 - val_mse: 15.4937 - val_mae: 2.7390\n"
+      "270/270 [==============================] - 0s 214us/sample - loss: 4.2883 - mse: 4.2883 - mae: 1.5662 - val_loss: 12.3803 - val_mse: 12.3803 - val_mae: 2.4916\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f315c319be0>"
+       "<tensorflow.python.keras.callbacks.History at 0x13c262f28>"
       ]
      },
-     "execution_count": 8,
+     "execution_count": 6,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "wandb.init(project=\"boston\", entity=\"lambda-ds7\") #Initializes and Experiment\n",
+    "#Initializes and Experiment\n",
+    "wandb.init(project=wandb_project, entity=wandb_group)\n",
     "\n",
     "# Important Hyperparameters\n",
     "X =  x_train\n",
@@ -930,7 +844,7 @@
     "model.add(Dense(64, activation='relu'))\n",
     "model.add(Dense(1))\n",
     "# Compile Model\n",
-    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
+    "model.compile(optimizer=Adam(learning_rate=0.02), loss='mse', metrics=['mse', 'mae'])\n",
     "\n",
     "# Fit Model\n",
     "model.fit(X, y, \n",
@@ -950,150 +864,6 @@
     "You will be expected to use Weights & Biases to try to tune your model during your module assignment today. "
    ]
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Hyperparameters with RandomSearchCV (Learn)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Overview\n",
-    "\n",
-    "Basically `GridSearchCV` takes forever. You'll want to adopt a slightly more sophiscated strategy."
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Follow Along"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "sweep_config = {\n",
-    "    'method': 'random',\n",
-    "    'parameters': {\n",
-    "        'learning_rate': {'distribution': 'normal'},\n",
-    "        'epochs': {'distribution': 'uniform',\n",
-    "                    'min': 100,\n",
-    "                    'max': 1000},\n",
-    "        'batch_size': {'distribution': 'uniform',\n",
-    "            'min': 10,\n",
-    "            'max': 400}\n",
-    "    }\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 13,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Create sweep with ID: huau0u9r\n",
-      "Sweep URL: https://app.wandb.ai/lambda-ds7/boston/sweeps/huau0u9r\n"
-     ]
-    }
-   ],
-   "source": [
-    "sweep_id = wandb.sweep(sweep_config)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 11,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import wandb\n",
-    "from wandb.keras import WandbCallback\n",
-    "#Initializes and Experiment\n",
-    "\n",
-    "from tensorflow.keras.optimizers import Adam\n",
-    "\n",
-    "# Important Hyperparameters\n",
-    "X =  x_train\n",
-    "y =  y_train\n",
-    "\n",
-    "inputs = X.shape[1]\n",
-    "\n",
-    "def train():\n",
-    "    \n",
-    "    wandb.init(project=\"boston\", entity=\"lambda-ds7\") \n",
-    "    \n",
-    "    config = wandb.config\n",
-    "\n",
-    "    # Create Model\n",
-    "    model = Sequential()\n",
-    "    model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
-    "    model.add(Dense(64, activation='relu'))\n",
-    "    model.add(Dense(64, activation='relu'))\n",
-    "    model.add(Dense(1))\n",
-    "\n",
-    "    # Optimizer \n",
-    "    adam = Adam(learning_rate=config.learning_rate)\n",
-    "\n",
-    "    # Compile Model\n",
-    "    model.compile(optimizer=adam, loss='mse', metrics=['mse', 'mae'])\n",
-    "\n",
-    "    # Fit Model\n",
-    "    model.fit(X, y, \n",
-    "              validation_split=0.33, \n",
-    "              epochs=config.epochs, \n",
-    "              batch_size=config.batch_size, \n",
-    "              callbacks=[WandbCallback()]\n",
-    "             )"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "wandb: Agent Starting Run: 2g77kp6k with config:\n",
-      "\tbatch_size: 308.503347845309\n",
-      "\tepochs: 704.9395850579006\n",
-      "\tlearning_rate: 1.480005523005428\n",
-      "wandb: Agent Started Run: 2g77kp6k\n"
-     ]
-    },
-    {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/t4w9l4ye</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "wandb.agent(sweep_id, function=train)"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
@@ -1122,10 +892,7 @@
     "    - Weights & Biases\n",
     "    - Comet.ml\n",
     "    - By Hand / GridSearch\n",
-    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch\n",
-    "    - Sklearn still useful (haha)\n",
-    "    - Integration with Wieghts & Biases\n",
-    "* <a href=\"#p4\">Part 4</a>: Discuss emerging hyperparameter tuning strategies\n",
+    "* <a href=\"#p3\">Optional</a>: Discuss emerging hyperparameter tuning strategies\n",
     "    - Bayesian Optimization\n",
     "    - Hyperopt\n",
     "    - Genetic Evolution"
@@ -1153,9 +920,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "conda_tensorflow_p36",
+   "display_name": "4-2",
    "language": "python",
-   "name": "conda_tensorflow_p36"
+   "name": "4-2"
   },
   "language_info": {
    "codemirror_mode": {
@@ -1167,7 +934,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.5"
+   "version": "3.7.0"
   }
  },
  "nbformat": 4,
